{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Web Scraping\n",
    "\n",
    "_Author: Dave Yerrington (SF)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Class\n",
    "\n",
    "#### Install Selenium\n",
    "\n",
    "Selenium is a headless browser. It allows us to render JavaScript just as a human-navigated browser would.\n",
    "\n",
    "To install Selenium, use one of the following:\n",
    "- **Anaconda:** `conda install -c conda-forge selenium`\n",
    "- **pip:** `pip install selenium`\n",
    "\n",
    "\n",
    "#### Install GeckoDriver\n",
    "\n",
    "You will also need GeckoDriver (this assumes you are using Homebrew for Mac): \n",
    "\n",
    "- ```brew install geckodriver```\n",
    "\n",
    "#### Install Firefox\n",
    "\n",
    "Additionally, you will need to have downloaded the [Firefox browser](https://www.mozilla.org/en-US/firefox/new/?utm_source=google&utm_medium=cpc&utm_campaign=Firefox-Brand-US-GGL-Exact&utm_term=firefox&utm_content=A144_A203_A006336&gclid=Cj0KEQjwnPLKBRC-j7nt1b7OlZwBEiQAv8lMLJUyReT6cPzSYdmEA6uD3YDoieuuuusddgAU7XH6smEaAoje8P8HAQ&gclsrc=aw.ds) for the application in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Revisit how to locate elements on a webpage\n",
    "- Aquire unstructure data from the internet using Beautiful soup.\n",
    "- Discuss limitations associated with simple requests and urllib libraries\n",
    "- Introduce Selenium as a solution, and implement a scraper using selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Guide\n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Building a web scraper](#building-scraper)\n",
    "- [Retrieving data from the HTML page](#retrieving-data)\n",
    "    - [Retrieving the restaurant names](#retrieving-names)\n",
    "    - [Challenge: Retrieving the restaurant locations](#retrieving-locations)\n",
    "    - [Retrieving the restaurant prices](#retrieving-prices)\n",
    "\n",
    "\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## Introduction\n",
    "\n",
    "In this codealong lesson, we'll build a web scraper using requests and BeautifulSoup. We will also explore how to use a headless browser called Selenium.\n",
    "\n",
    "We'll begin by scraping OpenTable's DC listings. We're interested in knowing the restaurant's **name, location, price, and how many people booked it today.**\n",
    "\n",
    "OpenTable provides all of this information on this given page: http://www.opentable.com/washington-dc-restaurant-listings\n",
    "\n",
    "Let's inspect the elements of this page to assure we can find each of the bits of information in which we're interested.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"building-scraper\"></a>\n",
    "## Building a web scraper\n",
    "\n",
    "Now, let's build a web scraper for OpenTable using urllib and Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import our necessary first packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the url we want to visit\n",
    "url = \"http://www.opentable.com/washington-dc-restaurant-listings\"\n",
    "\n",
    "# visit that url, and grab the html of said page\n",
    "html = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, what is in html?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .text returns the request content in Unicode\n",
    "html.text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to convert this html objct into a soup object so we can parse it using python and BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this into a soup object\n",
    "soup = BeautifulSoup(html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"retrieving-data\"></a>\n",
    "### Retrieving data from the HTML page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first find each restaurant name listed on the page we've loaded. How do we find the page location of the restaurant? (Hint: We need to know where in the **HTML** the restaurant element is housed.) In order to find the HTML that renders the restaurant location, we can use Google Chrome's Inspect tool:\n",
    "\n",
    "> http://www.opentable.com/washington-dc-restaurant-listings\n",
    "\n",
    "> 1. Visit the URL above. \n",
    "\n",
    "> 2. Right-click on an element you are interested in, then choose Inspect (in Chrome). \n",
    "\n",
    "> 3. This will open the Developer Tools and show the HTML used to render the selected page element. \n",
    "\n",
    "> Throughout this lesson, we will use this method to find tags associated with elements of the page we want to scrape.\n",
    "\n",
    "See if you can find the restaurant name on the page. Keep in mind there are many restaurants loaded on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the restaurant names\n",
    "soup.find_all(name='span', attrs={'class':'rest-row-name-text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to always keep in mind the data types that were returned. Note this is a `list`, and we know that immediately by observing the outer square brackets and commas separating each tag.\n",
    "\n",
    "Next, note the elements of the list are `Tag` objects, not strings. (If they were strings, they would be surrounded by quotes.) The Beautiful Soup authors chose to display a `Tag` object visually as a text representation of the tag and its contents. However, being an object, it has many methods that we can call on it. For example, next we will use the `encode_contents()` method to return the tag's contents encoded as a Python string.\n",
    "\n",
    "<a id=\"retrieving-names\"></a>\n",
    "#### Retrieving the restaurant names\n",
    "\n",
    "Now that we found a list of tags containing the restaurant names, let's think how we can loop through them all one-by-one. In the following cell, we'll print out the name (and **only** the clean name, not the rest of the html) of each restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each element you find, print out the restaurant name\n",
    "for entry in soup.find_all(name='span', attrs={'class':'rest-row-name-text'}):\n",
    "    print(entry.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "<a id=\"retrieving-locations\"></a>\n",
    "#### Challenge: Retrieving the restaurant locations\n",
    "\n",
    "Can you repeat that process for finding the location? For example, barmini by Jose Andres is in the location listed as \"Penn Quarter\" in our search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, see if you can identify the location for all elements -- print it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now print out EACH location for the restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"retrieving-prices\"></a>\n",
    "#### Retrieving the restaurant prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we've figured out the restaurant name and location. Now we need to grab the price (number of dollar signs on a scale of one to four) for each restaurant. We'll follow the same process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print out all prices\n",
    "soup.find_all('div', {'class':'rest-row-pricing'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out EACH number of dollar signs per restaurant\n",
    "# this one is trickier to eliminate the html. Hint: try a nested find\n",
    "for entry in soup.find_all('div', {'class':'rest-row-pricing'}):\n",
    "    print(entry.find('i').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks great, but what if I wanted just the number of dollar signs per restaurant? Can you figure out a way to simply print out the number of dollar signs per restaurant listed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of dollars signs per restaurant\n",
    "for entry in soup.find_all('div', {'class':'rest-row-pricing'}):\n",
    "    print( entry.find('i').text.count('$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's weird -- an empty set. Did we find the wrong element? What's going on here? Discuss.\n",
    "\n",
    "How can we debug this? Any ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"selenium\"></a>\n",
    "## Introducing Selenium\n",
    "\n",
    "Selenium is a headless browser. It allows us to render JavaScript just as a human-navigated browser would.\n",
    "\n",
    "To install Selenium, use one of the following:\n",
    "- **Anaconda:** `conda install -c conda-forge selenium`\n",
    "- **pip:** `pip install selenium`\n",
    "\n",
    "You will also need GeckoDriver (this assumes you are using Homebrew for Mac): \n",
    "\n",
    "- ```brew install geckodriver```\n",
    "\n",
    "Additionally, you will need to have downloaded the [Firefox browser](https://www.mozilla.org/en-US/firefox/new/?utm_source=google&utm_medium=cpc&utm_campaign=Firefox-Brand-US-GGL-Exact&utm_term=firefox&utm_content=A144_A203_A006336&gclid=Cj0KEQjwnPLKBRC-j7nt1b7OlZwBEiQAv8lMLJUyReT6cPzSYdmEA6uD3YDoieuuuusddgAU7XH6smEaAoje8P8HAQ&gclsrc=aw.ds) for the application in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium requires us to determine a default browser to run. I'm going to opt for Firefox, but Chromium is also a very common choice. http://selenium-python.readthedocs.io/faq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP\n",
    "# what is going to happen when I run the next cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path =\"Chromedriver\\geckodriver.exe\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a driver called Firefox\n",
    "driver = webdriver.Firefox(executable_path=driver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty crazy, right? Let's close that driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close it\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's boot it up, and visit a URL of our choice\n",
    "driver = webdriver.Firefox(executable_path=driver_path )\n",
    "driver.get(\"http://www.python.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close it\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Now we're getting somewhere: programmatically controlling our browser like a human."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to our problem at hand. We need to visit the OpenTable listing for DC. Once there, we need to get the html to load. In the next cell, prove you can programmatically visit the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit our OpenTable page\n",
    "driver = webdriver.Firefox(executable_path=driver_path )\n",
    "driver.get(\"http://www.opentable.com/washington-dc-restaurant-listings\")\n",
    "\n",
    "# always good to check we've got the page we think we do\n",
    "assert \"OpenTable\" in driver.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"selenium-js\"></a>\n",
    "### Running JavaScript before scraping\n",
    "\n",
    "Now, to resolve our JavaScript problem, there's a few things we can do. What I'll do in this case is request that the page load, wait one second, and then I'm going to grab the source html from the page. Because the page should believe I'm visiting from a live connection on a browser client, the JavaScript should render to be a part of the page source. I can then grab the page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sleep\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit our relevant page\n",
    "driver = webdriver.Firefox(executable_path=driver_path )\n",
    "driver.get(\"http://www.opentable.com/washington-dc-restaurant-listings\")\n",
    "\n",
    "# wait one second\n",
    "sleep(1)\n",
    "\n",
    "#grab the page source\n",
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pop Quiz:** What do we need to do with this HTML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup it!\n",
    "html = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's return to our earlier problem: How do we locate bookings on the page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the number bookings for all restaurants\n",
    "html.find_all('div', {'class':'booking'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now print out each booking for the listings using a loop\n",
    "for entry in html.find_all('div', {'class':'booking'}):\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab just the text of each of these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same as above, but grabbing only the text content\n",
    "for entry in html.find_all('div', {'class':'booking'}):\n",
    "    print(entry.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've succeeded!\n",
    "\n",
    "<a id=\"selenium-regex\"></a>\n",
    "### Using regex to only get digits\n",
    "\n",
    "But we can clean this up a little bit. We're going to use regular expressions (regex) to grab only the digits that are available in each of the text.\n",
    "\n",
    "The best way to get good at regex is to, well, just keep trying and testing: http://pythex.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we haven't covered regex, I'll show you how to use the search function to match any given digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each entry, grab the text\n",
    "for booking in html.find_all('div', {'class':'booking'}):\n",
    "    # match all digits\n",
    "    match = re.search(r'\\d+', booking.text)\n",
    "    \n",
    "    if match:\n",
    "        # print if found\n",
    "        print(match.group())\n",
    "    else:\n",
    "        # otherwise pass\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we demonstrate all the other amazing things about headless browsers, let's finish up collecting the data we want from this current example. Do you suppose the html parsing we wrote above will still work on the page source we've grabbed from our headless browser?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be most efficient, we want to only do a single loop for each entry on the page. That means we want to find what element all of other other elements (name, location, price, bookings) is housed within. Where on the page is each entry located?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out all entries\n",
    "#   NOTE: Has many entries. Uncomment the below code to run it!\n",
    "\n",
    "html.find_all('li', {'class':'result content-section-list-row cf with-times'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look over the page. Does every single entry have each element we're seeking?\n",
    "> I did this previously. I know for a fact that not every element has a number of recent bookings. That's probably exactly why OpenTable houses this in JavaScript: they want to continously update the number of bookings with the most relevant number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We want to only retrieve the text of the bookings.\n",
    "# But what would happen if we just naively print the text of each node?\n",
    "\n",
    "for entry in html.find_all('li', {'class':'result content-section-list-row cf with-times'}):\n",
    "    print(entry.find('div', {'class':'booking'}))   # try adding .text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we find the element we want, we print it. Otherwise, we print 'ZERO'\n",
    "for entry in html.find_all('li', {'class':'result content-section-list-row cf with-times'}):\n",
    "        divs = entry.find('div', {'class':'booking'})\n",
    "        \n",
    "        if divs:\n",
    "            print(divs.text)\n",
    "        else:\n",
    "            print('ZERO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice takes the place when booking is not found?\n",
    "\n",
    "We could use exception handling (`try`/`except` blocks) to resolve this. However, exceptions should only be used to handle rare or unexpected errors -- never for normal program flow.\n",
    "\n",
    "In this case, we expect that some entries will be zero. So, we can just use an `if` statement that tests whether there are any `divs` present; if not, display `'ZERO'`. Here's a demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we find the element we want, we print it. Otherwise, we print 'ZERO'\n",
    "for entry in html.find_all('li', {'class':'result content-section-list-row cf with-times'}):\n",
    "    booking_tag = entry.find('div', {'class':'booking'})\n",
    "    \n",
    "    if booking_tag:\n",
    "        print(booking_tag.text)\n",
    "    else:\n",
    "        print('ZERO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After previously completing this, we observed that all other elements WILL be returned. This means we do not have to always handle these cases.\n",
    "\n",
    "<a id=\"challenge-pandas\"></a>\n",
    "### Challenge: Use Pandas to create a DataFrame of bookings\n",
    "\n",
    "However, the onus is on you to now put all the pieces together.\n",
    "\n",
    "Loop through each entry. For each entry, grab the relevant information we want (name, location, price, bookings). Produce a dataframe with the columns \"name\",\"location\",\"price\",\"bookings\" that contains the 100 entries we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to create my empty df first\n",
    "dc_eats = pd.DataFrame(columns=[\"name\",\"location\",\"price\",\"bookings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out our work\n",
    "dc_eats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We succeeded.\n",
    "\n",
    "<a id=\"selenium-typing\"></a>\n",
    "### Auto-typing using Selenium\n",
    "\n",
    "Now, let's explore some of the other functionality of a webdriver. We've barely scratched the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can send keys as well\n",
    "# import\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open Firefox\n",
    "driver = webdriver.Firefox(executable_path=driver_path)\n",
    "# visit Python\n",
    "driver.get(\"http://www.python.org\")\n",
    "# verify we're in the right place\n",
    "assert \"Python\" in driver.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try automatedly typing `pycon` in the search box and hitting the return key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the search position\n",
    "elem = driver.find_element_by_name(\"q\")\n",
    "\n",
    "# clear it\n",
    "elem.clear()\n",
    "\n",
    "# type in pycon\n",
    "elem.send_keys(\"pycon\")\n",
    "\n",
    "# send those keys\n",
    "elem.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all at once:\n",
    "driver = webdriver.Firefox(executable_path=driver_path)\n",
    "driver.get(\"http://www.python.org\")\n",
    "assert \"Python\" in driver.title\n",
    "\n",
    "elem = driver.find_element_by_name(\"q\")\n",
    "elem.clear()\n",
    "elem.send_keys(\"pycon\")\n",
    "elem.send_keys(Keys.RETURN)\n",
    "#assert \"No results found.\" not in driver.page_source\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example (and many others) are available in the Selenium docs: http://selenium-python.readthedocs.io/getting-started.html\n",
    "\n",
    "What is especially important is exploring functionality like locating elements: http://selenium-python.readthedocs.io/locating-elements.html#locating-elements\n",
    "\n",
    "FAQ:\n",
    "http://selenium-python.readthedocs.io/faq.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this lesson, we used the Beautiful Soup library to locate elements on a website then scrape their text. We also used the Selenium headless browser to run JavaScript first before retrieving the page contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Exercise\n",
    "- For all CL housing listings on the front page, scrape the price and title and any other attributes into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this lesson, we used the Beautiful Soup library to locate elements on a website then scrape their text. We also used the Selenium headless browser to run JavaScript first before retrieving the page contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
