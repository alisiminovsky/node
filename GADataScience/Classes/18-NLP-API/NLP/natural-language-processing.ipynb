{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Natural Language Processing\n",
    " \n",
    "_Authors: Kiefer Katovich (San Francisco), Joseph Nelson (Washington, D.C.)_\n",
    " \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Discuss the major tasks involved with natural language processing.\n",
    "- Discuss, on a low level, the components of natural language processing.\n",
    "- Identify why natural language processing is difficult.\n",
    "- Demonstrate text classification.\n",
    "- Demonstrate common text preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Do We Use NLP in Data Science?\n",
    "\n",
    "In data science, we are often asked to analyze unstructured text or make a predictive model using it. Unfortunately, most data science techniques require numeric data. NLP libraries provide a tool set of methods to convert unstructured text into meaningful numeric data.\n",
    "\n",
    "- **Analysis:** NLP techniques provide tools to allow us to understand and analyze large amounts of text. For example:\n",
    "\n",
    "    - Analyze the positivity/negativity of comments on different websites. \n",
    "    - Extract key words from meeting notes and visualize how meeting topics change over time.\n",
    "\n",
    "- **Vectorizing for machine learning:** When building a machine learning model, we typically must transform our data into numeric features. This process of transforming non-numeric data such as natural language into numeric features is called vectorization. For example:\n",
    "\n",
    "    - Understanding related words. Using stemming, NLP lets us know that \"swim\", \"swims\", and \"swimming\" all refer to the same base word. This allows us to reduce the number of features used in our model.\n",
    "    - Identifying important and unique words. Using TF-IDF (term frequency-inverse document frequency), we can identify which words are most likely to be meaningful in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='textblob_install'></a>\n",
    "\n",
    "### Install TextBlob\n",
    "\n",
    "The TextBlob Python library provides a simplified interface for exploring common NLP tasks including part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "\n",
    "To proceed with the lesson, first install TextBlob, as explained below. We tend to prefer Anaconda-based installations, since they tend to be tested with our other Anaconda packages.\n",
    "\n",
    "**To install textblob run:**\n",
    "\n",
    "> `conda install -c conda-forge textblob`\n",
    "\n",
    "**Or:**\n",
    "\n",
    "> `pip install textblob`\n",
    "\n",
    "> `python -m textblob.download_corpora lite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Guide\n",
    "\n",
    "- [Introduction to Natural Language Processing](#intro)\n",
    "- [Reading Yelp reviews With NLP](#yelp_rev)\n",
    "- [Text Classification](#text_class)\n",
    "- [Count Vectorization](#count_vec)\n",
    "    - [Using CountVectorizer in a Model](#countvectorizer-model)\n",
    "    - [N-Grams](#ngrams)\n",
    "    - [Stop-Word Removal](#stopwords)\n",
    "\t- [Count Vector Options](#cvec_opt)\n",
    "- [Intro to TextBlob](#textblob)\n",
    "\t- [Stemming and Lemmatization](#stem)\n",
    "- [Term Frequency–Inverse Document Frequency Vectorization](#tfidf)\n",
    "\t- [Yelp Summary Using TF–IDF](#yelp_tfidf)\n",
    "- [Sentiment Analysis](#sentiment)\n",
    "- [BONUS: Adding Features to a Document-Term Matrix](#add_feat)\n",
    "- [BONUS: More TextBlob Features](#more_textblob)\n",
    "- [APPENDIX: Intro to Naive Bayes and Text Classification](#bayes)\n",
    "- [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is Natural Language Processing (NLP)?\n",
    "\n",
    "- Using computers to process (analyze, understand, generate) natural human languages.\n",
    "- Making sense of human knowledge stored as unstructured text.\n",
    "- Building probabilistic models using data about a language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Some of the Higher-Level Task Areas?\n",
    "\n",
    "- **Objective:** Discuss the major tasks involved with natural language processing.\n",
    "\n",
    "We often hope that computers can solve many high-level problems involving natural language. Unfortunately, due to the difficulty of understanding human language, many of these problems are still not well solved. That said, existing solutions to these problems all involve utilizing the lower-level components of NLP discussed in the next section. Some higher-level tasks include:\n",
    "\n",
    "- **Chatbots:** Understand natural language from the user and return intelligent responses.\n",
    "    - [Api.ai](https://api.ai/)\n",
    "- **Information retrieval:** Find relevant results and similar results.\n",
    "    - [Google](https://www.google.com/)    \n",
    "- **Information extraction:** Structured information from unstructured documents.\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation:** One language to another.\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification:** Preserve the meaning of text, but simplify the grammar and vocabulary.\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input:** Faster or easier typing.\n",
    "    - [Phrase completion application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis:** Attitude of speaker.\n",
    "    - [Hater News](https://medium.com/@KevinMcAlear/building-hater-news-62062c58325c)\n",
    "- **Automatic summarization:** Extractive or abstractive summarization.\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural language generation:** Generate text from data.\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation:** Speech-to-text, text-to-speech.\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering:** Determine the intent of the question, match query with knowledge base, evaluate hypotheses.\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Some of the Lower-Level Components?\n",
    "\n",
    "- **Objective:** Discuss, on a low level, the components of natural language processing.\n",
    "\n",
    "Unfortunately, the NLP programming libraries typically do not provide direct solutions for the high-level tasks above. Instead, they provide low-level building blocks that enable us to craft our own solutions. These include:\n",
    "\n",
    "- **Tokenization:** Breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stop-word removal:** a/an/the\n",
    "- **Stemming and lemmatization:** root word\n",
    "- **TF-IDF:** word importance\n",
    "- **Part-of-speech tagging:** noun/verb/adjective\n",
    "- **Named entity recognition:** person/organization/location\n",
    "- **Spelling correction:** \"New Yrok City\"\n",
    "- **Word sense disambiguation:** \"buy a mouse\"\n",
    "- **Segmentation:** \"New York City subway\"\n",
    "- **Language detection:** \"translate this page\"\n",
    "- **Machine learning:** specialized models that work well with text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "- **Objective:** Identify why natural language processing is difficult.\n",
    "\n",
    "Natural language processing requires an understanding of the language and the world. Several limitations of NLP are:\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals Are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English:** text messages\n",
    "- **Idioms:** \"throw in the towel\"\n",
    "- **Newly coined words:** \"retweet\"\n",
    "- **Tricky entity names:** \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge:** \"Mary and Sue are sisters\", \"Mary and Sue are mothers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yelp_rev'></a>\n",
    "\n",
    "## Reading in the Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this lesson, we will use Yelp reviews to practice and discover common low-level NLP techniques.\n",
    "\n",
    "You should be familiar with these terms, as they are frequently used in NLP:\n",
    "- **corpus**: a collection of documents (derived from the Latin word for \"body\")\n",
    "- **corpora**: plural form of corpus\n",
    "\n",
    "Throughout this lesson, we will use a model very popular for text classification called Naive Bayes (the \"NB\" in `BinonmialNB` and `MultinomialNB` below). If you are unfamiliar with it, know that it works exactly the same as all other models in scikit-learn! We will look extensively at the mechanics behind Naive Bayes later in the course. However, see the [appendix](#bayes) at the end of this notebook for a quick introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB         # Naive Bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read yelp.csv into a DataFrame.\n",
    "path = r'./data/yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "\n",
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# Define X and y.\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# Split the new DataFrame into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The head of the original data\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6841    5\n",
       "1728    5\n",
       "3853    5\n",
       "671     1\n",
       "4920    5\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='text_class'></a>\n",
    "\n",
    "\n",
    "# Introduction: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you proceed through this section, note that text classification is done in the same way as all other classification models. First, the text is vectorized into a set of numeric features. Then, a standard machine learning classifier is applied. NLP libraries often include vectorizers and ML models that work particularly well with text.\n",
    "\n",
    "> We will refer to each piece of text we are trying to classify as a document.\n",
    "> - For example, a document could refer to an email, book chapter, tweet, article, or text message.\n",
    "\n",
    "**Text classification is the task of predicting which category or topic a text sample is from.**\n",
    "\n",
    "We may want to identify:\n",
    "- Is an article a sports or business story?\n",
    "- Does an email have positive or negative sentiment?\n",
    "- Is the rating of a recipe 1, 2, 3, 4, or 5 stars?\n",
    "\n",
    "**Predictions are often made by using the words as features and the label as the target output.**\n",
    "\n",
    "Starting out, we will make each unique word (across all documents) a single feature. In any given corpora, we may have hundreds of thousands of unique words, so we may have hundreds of thousands of features!\n",
    "\n",
    "- For a given document, the numeric value of each feature could be the number of times the word appears in the document.\n",
    "    - So, most features will have a value of zero, resulting in a sparse matrix of features.\n",
    "\n",
    "- This technique for vectorizing text is referred to as a bag-of-words model. \n",
    "    - It is called bag of words because the document's structure is lost — as if the words are all jumbled up in a bag.\n",
    "    - The first step to creating a bag-of-words model is to create a vocabulary of all possible words in the corpora.\n",
    "\n",
    "> Alternatively, we could make each column an indicator column, which is 1 if the word is present in the document (no matter how many times) and 0 if not. This vectorization could be used to reduce the importance of repeated words. For example, a website search engine would be susceptible to spammers who load websites with repeated words. So, the search engine might use indicator columns as features rather than word counts.\n",
    "\n",
    "**We need to consider several things to decide if bag-of-words is appropriate.**\n",
    "\n",
    "- Does order of words matter?\n",
    "- Does punctuation matter?\n",
    "- Does upper or lower case matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Text Processing in scikit-learn\n",
    "\n",
    "- **Objective:** Demonstrate text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='count_vec'></a>\n",
    "\n",
    "\n",
    "### Creating Features Using CountVectorizer\n",
    "\n",
    "- **What:** Converts each document into a set of words and their counts.\n",
    "- **Why:** To use a machine learning model, we must convert unstructured text into numeric features.\n",
    "- **Notes:** Relatively easy with English language text, not as easy with some languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6841    FILLY-B's!!!!!  only 8 reviews?? NINE now!!!\\n...\n",
       "1728    My husband and I absolutely LOVE this restaura...\n",
       "3853    We went today after lunch. I got my usual of l...\n",
       "671     Totally dissapointed.  I had purchased a coupo...\n",
       "4920    Costco Travel - My husband and I recently retu...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to create document-term matrices from X_train and X_test.\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064, 16825)\n",
      "(1022, 16825)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16825)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rows are documents, columns are terms (aka \"tokens\" or \"features\", individual words in this situation).\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unusually', 'unvalidated', 'unwanted', 'unwarmed', 'unwelcome', 'unwilling', 'unwind', 'unyielding', 'uopx', 'up', 'upbeat', 'upbringings', 'upc', 'upcharge', 'upcharging', 'upchuck', 'upcoming', 'update', 'updated', 'updates', 'updating', 'upfront', 'upgrade', 'upgraded', 'upgrades', 'upgrading', 'uphill', 'upholstery', 'uploaded', 'upo', 'upon', 'upper', 'uppers', 'upping', 'uprooted', 'ups', 'upscale', 'upset', 'upsets', 'upside', 'upstaged', 'upstairs', 'uptight', 'uptown', 'ur', 'urban', 'urbanspoon', 'urchin', 'urge', 'urgent', 'urinal', 'urinates', 'urine', 'uruguay', 'us', 'usa', 'usage', 'usain', 'usairways', 'use', 'used', 'useful', 'useless', 'user', 'uses', 'using', 'usl', 'usps', 'usual', 'usually', 'usuals', 'utah', 'utensil', 'utensils', 'uterus', 'utility', 'utilize', 'utilizing', 'utmost', 'utter', 'uttered', 'utterly', 'uv', 'ux10', 'uye', 'va', 'vaca', 'vacant', 'vacation', 'vacationing', 'vacations', 'vacay', 'vaccilate', 'vaccination', 'vaccines', 'vacuum', 'vacuumed', 'vaguely', 'vain', 'val', 'valencia', 'valentine', 'valentines', 'valerie', 'valet', 'valeted', 'valets', 'valid', 'validated', 'validation', 'valle', 'valley', 'valomilk', 'valuable', 'value', 'values', 'vampire', 'vampiro', 'van', 'vancouver', 'vandalized', 'vang', 'vanilla', 'vanity', 'vans', 'vantage', 'vaporized', 'variance', 'variation', 'variations', 'varied', 'varietals', 'varieties', 'variety', 'various', 'vary', 'vases', 'vast', 'vat', 'vatican', 'vatoci', 'vatra', 'vault', 've', 'veal', 'veer', 'veerrrrrryyyyyyyyy', 'veg', 'vegan', 'veganized', 'vegas', 'vegetable', 'vegetables', 'vegetarian', 'vegetarians', 'veggie', 'veggies', 'vegie', 'vegiies', 'vegtable', 'vehicle', 'vehicles', 'veiled', 'vein', 'veins', 'velvet', 'velveta', 'velvety', 'vender', 'vendor', 'vendors', 'venerable', 'venice', 'venom', 'vent', 'ventilation', 'vents', 'venture', 'ventured', 'ventures', 'venue', 'venues', 'venyse', 'vera', 'verbal', 'verbally', 'verde', 'verdes', 'verdict', 'verduras', 'verge', 'verges', 'verisimilitude', 'veritable', 'verizon', 'vermicelli', 'vermont', 'vermonter', 'veronica', 'veronique', 'verrado', 'verse', 'versed', 'version', 'versions', 'versus', 'verts', 'very', 'vests', 'vet', 'veterans', 'veterinary', 'vets', 'vexing', 'vey', 'via', 'viad', 'viagra', 'vibe', 'vibrant', 'vibrate', 'vibrated', 'vic', 'vice', 'vicinity', 'victorian', 'vide', 'video', 'videogame', 'videos', 'vids', 'vieja', 'viejas', 'vien', 'viet', 'vietnam', 'vietnamese', 'vietri', 'view', 'viewing', 'views', 'vig', 'vigazz', 'vignettes', 'viking', 'vikings', 'villa', 'village', 'villas', 'vin', 'vinagerette', 'vinagrette', 'vinaigrette', 'vincent', 'vindaloo', 'vine', 'vinegar', 'vinegarette', 'vinegary', 'vines', 'vineyard', 'vingerette', 'vings', 'vino', 'vintage', 'vinyl', 'viola', 'violated', 'violation', 'violations', 'violence', 'violently', 'vip', 'vips', 'virgin', 'virginia', 'virus', 'viscous', 'visibility', 'visible', 'visibly', 'vision', 'visit', 'visited', 'visiting', 'visitor', 'visitors', 'visits', 'vista', 'vistal', 'visualize', 'visually', 'visuals', 'vitality', 'vitamin', 'vitamins', 'vittles', 'vivant', 'vivid', 'vochos', 'vodka', 'vodkas', 'voice', 'voiced', 'void', 'voided', 'voila', 'volcano', 'volleyball', 'volume', 'volumes', 'volunteer', 'volunteered', 'volunteering', 'volunteers', 'vomit', 'von', 'voodoo', 'voracious', 'vortex', 'vote', 'voted', 'voting', 'vouch', 'voucher', 'vouchers', 'vow', 'vowed', 'vs', 'vsc', 'vueve', 'vulgar', 'vulture', 'vultures', 'vw', 'wa', 'waaaaaaaaay', 'waaaaaay', 'waayy', 'wac', 'wach', 'wacky', 'wad', 'waders', 'waffle', 'waffles', 'waffling', 'wag', 'wager', 'wagged', 'wagyu', 'waikiki', 'waist', 'waisting', 'wait', 'waited', 'waiter', 'waiters', 'waiting', 'waitress', 'waitresses', 'waits', 'waitstaff', 'waitstand', 'waived', 'wake', 'wal', 'waldorf', 'walet', 'walgreens', 'walk', 'walkable', 'walked', 'walking', 'walks', 'walkway', 'walkways', 'wall', 'wallet', 'wallets', 'wallowing', 'wallpaper', 'wallpapers', 'walls', 'wally', 'walmart', 'walnut', 'walnuts', 'waltzed', 'wand', 'wander', 'wandered', 'wandering', 'wanna', 'wannabes', 'want', 'want_', 'wanted', 'wanting', 'wantons', 'wants', 'war', 'ward', 'wardrobe', 'wards', 'ware', 'warehouse', 'warehoused', 'warm', 'warmed', 'warmer', 'warmers', 'warming', 'warmly', 'warmth', 'warn', 'warned', 'warner', 'warning', 'warnings', 'warp', 'warrant', 'warrants', 'warranty', 'warriors', 'wars', 'wary', 'was', 'wasabe', 'wasabi', 'wash', 'washcloths', 'washday', 'washed', 'washer', 'washers', 'washes', 'washing', 'washington', 'washingtons', 'wasn', 'wasnt', 'wasp', 'waste', 'wastebaskets', 'wasted', 'waster', 'wasting', 'wasy', 'watch', 'watched', 'watcher', 'watchers', 'watches', 'watching', 'water', 'watered', 'waterfall', 'waterfalls', 'waterfront', 'watering', 'watermelon', 'waterproof', 'waters', 'waterslides', 'waterworks', 'watery', 'watt', 'wave', 'waved', 'wavelle', 'waving', 'wax', 'waxed', 'waxing', 'way', 'wayne', 'ways', 'wayward', 'wbc', 'wcg', 'we', 'weak', 'weakness', 'wealthier', 'weapon', 'wear', 'wearing', 'wears', 'weary', 'weather', 'weaving', 'web', 'website', 'websites', 'wed', 'wedding', 'weddings', 'wedge', 'wedges', 'wednesday', 'wednesdays', 'weds', 'wee', 'week', 'weekday', 'weekdays', 'weekend', 'weekends', 'weekly', 'weeknight', 'weeknights', 'weeks', 'wei', 'weigh', 'weighs', 'weight', 'weighted', 'weights', 'weihenstephaner', 'weil', 'weird', 'weirded', 'weirdest', 'weirdly', 'weirdo', 'weirdos', 'welcome', 'welcomed', 'welcomes', 'welcoming', 'well', 'wellness', 'wells', 'wendy', 'wendys', 'went', 'were', 'weren', 'werent', 'west', 'western', 'westgate', 'westin', 'westminster', 'westside', 'westward', 'wet', 'whaat', 'whack', 'whacked', 'whales', 'what', 'whatever', 'whatnot', 'whats', 'whatsoever', 'whatsosever', 'whatsover', 'wheat', 'wheel', 'wheeling', 'wheels', 'wheezed', 'when', 'whenever', 'where', 'whereupon', 'wherever', 'whether', 'whew', 'which', 'whiff', 'while', 'whilst', 'whim', 'whimsical', 'whines', 'whinin', 'whining', 'whinnying', 'whip', 'whipped', 'whips', 'whirl', 'whirlpools', 'whisked', 'whiskey', 'whiskeys', 'whisper', 'whispered', 'whispers', 'whistling', 'white', 'whiten', 'whites', 'whitesands', 'whitey', 'whitney', 'whiz', 'who', 'whoa', 'whoah', 'whoever', 'whole', 'wholeheartedly', 'wholly', 'whom', 'whooping', 'whopper', 'whopping', 'whore', 'whoreish', 'whoring', 'whose', 'why', 'wi', 'wich', 'wicked', 'wide', 'wider', 'widow', 'wiener', 'wierd', 'wife', 'wifey', 'wifi', 'wig', 'wiggle', 'wigs', 'wigwam', 'wih', 'wii', 'wiki', 'wikipedia', 'wil', 'wild', 'wildcat', 'wilderness', 'wildest', 'wildfish', 'wildflower', 'wildhorse', 'wildlife', 'wildly', 'will', 'willie', 'willing', 'willingfully', 'willow', 'willpower', 'wills', 'willy', 'willys', 'wilma', 'wilted', 'wimp', 'wimper', 'win', 'wind', 'window', 'windowpanes', 'windows', 'winds', 'windshield', 'windsor', 'windy', 'wine', 'wines', 'wing', 'wings', 'wink', 'winkie', 'winking', 'winky', 'winner', 'winners', 'winning', 'wino', 'winos', 'wins', 'winter', 'wipe', 'wiped', 'wiper', 'wipes', 'wiping', 'wire', 'wireless', 'wiring', 'wisconsin', 'wisdom', 'wise', 'wiseguy', 'wisely', 'wiser', 'wish', 'wished', 'wishful', 'wishing', 'witches', 'with', 'withdrawal', 'withheld', 'within', 'without', 'withstanding', 'witness', 'witnessed', 'wives', 'wiz', 'wizard', 'wks', 'wll', 'wobble', 'wobbly', 'woes', 'wok', 'woke', 'woken', 'wolfed', 'wolfpack', 'wolverine', 'woman', 'women', 'womens', 'won', 'wonder', 'wondered', 'wonderful', 'wonderfully', 'wonderfulness', 'wondering', 'wonderland', 'wonderous', 'wonders', 'wondrous', 'wongs', 'wont', 'wonton', 'wontons', 'woo', 'wood', 'wooden', 'woodlands', 'woods', 'woodworker', 'wooed', 'woohoo', 'wool', 'word', 'wordly', 'words', 'wore', 'work', 'worked', 'worker', 'workers', 'working', 'workmanship', 'workout', 'workouts', 'works', 'world', 'worlds', 'worldwide', 'worm', 'worn', 'worried', 'worries', 'worry', 'worrying', 'worse', 'worship', 'worst', 'worstershire', 'worth', 'worthless', 'worthwhile', 'worthy', 'wott', 'would', 'woulda', 'wouldn', 'wouldnt', 'wouldve', 'wound', 'wow', 'wowed', 'wowzer', 'wracking', 'wrap', 'wrapped', 'wrapper', 'wrappers', 'wrapping', 'wraps', 'wrath', 'wreck', 'wrestling', 'wrigley', 'wrigleyville', 'wrinkled', 'wrinkles', 'wrinkling', 'wrist', 'wristbands', 'write', 'writer', 'writers', 'writes', 'writing', 'written', 'wrkout', 'wrong', 'wrote', 'wtf', 'wth', 'wussies', 'wwe', 'www', 'würze', 'x2', 'xanax', 'xbox', 'xeriscape', 'xg8u6fz2e8hu0xq3hf7czg', 'xl', 'xlv', 'xmas', 'ya', 'yack', 'yah', 'yahoo', 'yakaguru', 'yakisoba', 'yall', 'yam', 'yang', 'yankin', 'yanks', 'yard', 'yards', 'yardsale', 'yasu', 'yawning', 'yay', 'yc', 'yea', 'yeah', 'year', 'yearly', 'yearning', 'yearnings', 'years', 'yeh', 'yell', 'yelled', 'yelling', 'yellow', 'yellowfin', 'yellowtail', 'yells', 'yelp', 'yelped', 'yelper', 'yelpers', 'yelping', 'yen', 'yep', 'yeppie', 'yer', 'yerba', 'yes', 'yesssss', 'yesterday', 'yet', 'yew', 'yielding', 'yikes', 'yin', 'yippee', 'ymca', 'yo', 'yodeling', 'yodels', 'yoga', 'yogalates', 'yogis', 'yogurt', 'yogurtini', 'yogurtland', 'yogurtology', 'yogurtopia', 'yogurts', 'yoli', 'yolk', 'yon', 'yoohoo', 'york', 'yorker', 'yorkie', 'you', 'youki', 'young', 'younger', 'youngest', 'your', 'youre', 'yours', 'yourself', 'yourselves', 'youth', 'youthful', 'youtube', 'yow', 'yowza', 'yr', 'yragui', 'yrs', 'yu', 'yuck', 'yucky', 'yuk', 'yukgejang', 'yukon', 'yum', 'yuma', 'yumm', 'yummie', 'yummier', 'yumminess', 'yummm', 'yummmm', 'yummmmmm', 'yummmmmmers', 'yummmmy', 'yummy', 'yumness', 'yung', 'yup', 'yupha', 'yuppies', 'yusefs', 'yuukk', 'yuuuummmmae', 'yuuuuummmmmyyy', 'yuuuuuuum', 'yuyuyummy', 'yuzu', 'yyyyy', 'z11', 'za', 'zabba', 'zach', 'zam', 'zanella', 'zankou', 'zappos', 'zatsiki', 'zen', 'zero', 'zest', 'zexperience', 'zha', 'zhou', 'zia', 'zihuatenejo', 'zilch', 'zin', 'zinburger', 'zinburgergeist', 'zinc', 'zinfandel', 'zing', 'zip', 'zipcar', 'zipper', 'zippers', 'zipps', 'ziti', 'zoe', 'zombi', 'zombies', 'zone', 'zones', 'zoning', 'zoo', 'zoyo', 'zucca', 'zucchini', 'zuchinni', 'zumba', 'zupa', 'zuzu', 'zwiebel', 'zzed', 'éclairs', 'école', 'ém']\n"
     ]
    }
   ],
   "source": [
    "# Last 50 features\n",
    "print((vect.get_feature_names()[-1000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CountVectorizer in module sklearn.feature_extraction.text object:\n",
      "\n",
      "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |  \n",
      " |  Convert a collection of text documents to a matrix of token counts\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}, default='content'\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be a sequence of items that\n",
      " |      can be of type string or byte.\n",
      " |  \n",
      " |  encoding : string, default='utf-8'\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode'}, default=None\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |  \n",
      " |  lowercase : bool, default=True\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  preprocessor : callable, default=None\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  tokenizer : callable, default=None\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, default=None\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      word n-grams or char n-grams to be extracted. All values of n such\n",
      " |      such that min_n <= n <= max_n will be used. For example an\n",
      " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char', 'char_wb'} or callable,             default='word'\n",
      " |      Whether the feature should be made of word n-gram or character\n",
      " |      n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |  \n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, default=None\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : bool, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : type, default=np.int64\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  fixed_vocabulary_: boolean\n",
      " |      True if a fixed vocabulary of term to indices mapping\n",
      " |      is provided by the user\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = CountVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> print(vectorizer.get_feature_names())\n",
      " |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      " |  >>> print(X.toarray())\n",
      " |  [[0 1 1 1 0 0 1 0 1]\n",
      " |   [0 2 0 1 0 1 1 0 1]\n",
      " |   [1 0 0 1 1 0 1 1 1]\n",
      " |   [0 1 1 1 0 0 1 0 1]]\n",
      " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
      " |  >>> print(vectorizer2.get_feature_names())\n",
      " |  ['and this', 'document is', 'first document', 'is the', 'is this',\n",
      " |  'second document', 'the first', 'the second', 'the third', 'third one',\n",
      " |   'this document', 'this is', 'this the']\n",
      " |   >>> print(X2.toarray())\n",
      " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HashingVectorizer, TfidfVectorizer\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      _VectorizerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return document-term matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names : list\n",
      " |          A list of feature names.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays of shape (n_samples,)\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing, tokenization\n",
      " |      and n-grams generation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      analyzer: callable\n",
      " |          A function to handle preprocessing, tokenization\n",
      " |          and n-grams generation.\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      preprocessor: callable\n",
      " |            A function to preprocess the text before tokenization.\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tokenizer: callable\n",
      " |            A function to split a string into a sequence of tokens.\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols.\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : str\n",
      " |          The string to decode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc: str\n",
      " |          A string of unicode symbols.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stop_words: list or None\n",
      " |              A list of stop words.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show vectorizer options.\n",
    "help(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dtm = pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00a</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>03342</th>\n",
       "      <th>04</th>\n",
       "      <th>...</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuchinni</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zupa</th>\n",
       "      <th>zuzu</th>\n",
       "      <th>zwiebel</th>\n",
       "      <th>zzed</th>\n",
       "      <th>éclairs</th>\n",
       "      <th>école</th>\n",
       "      <th>ém</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3062</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3064 rows × 16825 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  00a  00am  00pm  01  02  03  03342  04  ...  zucchini  \\\n",
       "0      0    0    0     0     0   0   0   0      0   0  ...         0   \n",
       "1      0    0    0     0     0   0   0   0      0   0  ...         0   \n",
       "2      0    0    0     0     0   0   0   0      0   0  ...         0   \n",
       "3      0    0    0     0     0   0   0   0      0   0  ...         0   \n",
       "4      0    0    0     0     0   0   0   0      0   0  ...         0   \n",
       "...   ..  ...  ...   ...   ...  ..  ..  ..    ...  ..  ...       ...   \n",
       "3059   0    0    0     0     0   0   0   0      0   0  ...         0   \n",
       "3060   0    0    0     0     0   0   0   0      0   0  ...         0   \n",
       "3061   0    0    0     0     0   0   0   0      0   0  ...         0   \n",
       "3062   0    0    0     0     0   0   0   0      0   0  ...         0   \n",
       "3063   0    0    0     0     0   0   0   0      0   0  ...         0   \n",
       "\n",
       "      zuchinni  zumba  zupa  zuzu  zwiebel  zzed  éclairs  école  ém  \n",
       "0            0      0     0     0        0     0        0      0   0  \n",
       "1            0      0     0     0        0     0        0      0   0  \n",
       "2            0      0     0     0        0     0        0      0   0  \n",
       "3            0      0     0     0        0     0        0      0   0  \n",
       "4            0      0     0     0        0     0        0      0   0  \n",
       "...        ...    ...   ...   ...      ...   ...      ...    ...  ..  \n",
       "3059         0      0     0     0        0     0        0      0   0  \n",
       "3060         0      0     0     0        0     0        0      0   0  \n",
       "3061         0      0     0     0        0     0        0      0   0  \n",
       "3062         0      0     0     0        0     0        0      0   0  \n",
       "3063         0      0     0     0        0     0        0      0   0  \n",
       "\n",
       "[3064 rows x 16825 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common method of reducing the number of features is converting all text to lowercase before generating features! Note that to a computer, `aPPle` is a different token/\"word\" than `apple`. So, by converting both to lowercase letters, it ensures fewer features will be generated. It might be useful not to convert them to lowercase if capitalization matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16825)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 20838)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't convert to lowercase.\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape\n",
    "#vect.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect.get_feature_names()[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='countvectorizer-model'></a>\n",
    "\n",
    "\n",
    "### Using CountVectorizer in a Model\n",
    "![DTM](images/DTM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9187866927592955\n"
     ]
    }
   ],
   "source": [
    "# Use default options for CountVectorizer.\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Create document-term matrices.\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# Use Naive Bayes to predict the star rating.\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# Calculate accuracy.\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    838\n",
       "1    184\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent 5 Stars: 0.8199608610567515\n",
      "Percent 1 Stars: 0.18003913894324852\n"
     ]
    }
   ],
   "source": [
    "# Calculate null accuracy.\n",
    "y_test_binary = np.where(y_test==5, 1, 0) # five stars become 1, one stars become 0\n",
    "print('Percent 5 Stars:', y_test_binary.mean())\n",
    "print('Percent 1 Stars:', 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicted ~92% accuracy, which is an improvement over this baseline 82% accuracy (assuming our model always predicts 5 stars).\n",
    "\n",
    "Let's look more into how the vectorizer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3064x16825 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 237720 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice how the data was transformed into this sparse matrix with 1,022 datapoints and 16,825 features!\n",
    "#   - Recall that vectorizations of text will be mostly zeros, since only a few unique words are in each document.\n",
    "#   - For that reason, instead of storing all the zeros we only store non-zero values (inside the 'sparse matrix' data structure!).\n",
    "#   - We have 3064 Yelp reviews in our training set.\n",
    "#   - 16,825 unique words were found across all documents.\n",
    "\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filly': 5773,\n",
       " 'only': 10362,\n",
       " 'reviews': 12465,\n",
       " 'nine': 10069,\n",
       " 'now': 10180,\n",
       " 'wow': 16612,\n",
       " 'do': 4631,\n",
       " 'miss': 9578,\n",
       " 'this': 15093,\n",
       " 'place': 11186,\n",
       " '24hrs': 136,\n",
       " 'drive': 4809,\n",
       " 'thru': 15136,\n",
       " 'or': 10413,\n",
       " 'walk': 16195,\n",
       " 'up': 15834,\n",
       " 'ridiculously': 12514,\n",
       " 'cheap': 2789,\n",
       " 'tasty': 14838,\n",
       " 'of': 10286,\n",
       " 'course': 3679,\n",
       " 'the': 15032,\n",
       " 'arizona': 1018,\n",
       " 'burritos': 2286,\n",
       " 'are': 1003,\n",
       " 'good': 6571,\n",
       " 'everything': 5342,\n",
       " 'is': 7956,\n",
       " 'used': 15885,\n",
       " 'to': 15228,\n",
       " 'love': 8899,\n",
       " 'one': 10354,\n",
       " 'combos': 3233,\n",
       " 'you': 16727,\n",
       " 'get': 6433,\n",
       " 'beef': 1564,\n",
       " 'burrito': 2285,\n",
       " 'taco': 14720,\n",
       " 'rice': 12489,\n",
       " 'and': 805,\n",
       " 'beans': 1528,\n",
       " 'for': 6028,\n",
       " 'under': 15683,\n",
       " 'color': 3213,\n",
       " 'me': 9301,\n",
       " 'silly': 13462,\n",
       " 'call': 2398,\n",
       " 'sally': 12808,\n",
       " 'they': 15067,\n",
       " 'have': 7023,\n",
       " 'bomb': 1902,\n",
       " 'horchata': 7345,\n",
       " 'too': 15281,\n",
       " 'really': 12038,\n",
       " 'fresh': 6154,\n",
       " 'flautas': 5882,\n",
       " 'rolled': 12622,\n",
       " 'tacos': 14721,\n",
       " 'breakfast': 2069,\n",
       " 'damn': 3999,\n",
       " 'here': 7149,\n",
       " 'whether': 16394,\n",
       " 'drunk': 4837,\n",
       " 'sober': 13745,\n",
       " 'my': 9868,\n",
       " 'husband': 7476,\n",
       " 'absolutely': 353,\n",
       " 'restaurant': 12410,\n",
       " 'anytime': 889,\n",
       " 'find': 5790,\n",
       " 'myself': 9871,\n",
       " 'craving': 3757,\n",
       " 'mexican': 9440,\n",
       " 'food': 6006,\n",
       " 'first': 5828,\n",
       " 'that': 15027,\n",
       " 'pops': 11347,\n",
       " 'in': 7619,\n",
       " 'head': 7042,\n",
       " 'salsa': 12814,\n",
       " 'blanca': 1780,\n",
       " 'we': 16306,\n",
       " 'always': 735,\n",
       " 'encountered': 5144,\n",
       " 'friendly': 6171,\n",
       " 'welcoming': 16355,\n",
       " 'staff': 14107,\n",
       " 'amazing': 752,\n",
       " 'fulfilling': 6251,\n",
       " 'what': 16377,\n",
       " 'more': 9735,\n",
       " 'could': 3655,\n",
       " 'ask': 1101,\n",
       " 'went': 16361,\n",
       " 'today': 15237,\n",
       " 'after': 563,\n",
       " 'lunch': 8945,\n",
       " 'got': 6605,\n",
       " 'usual': 15893,\n",
       " 'lime': 8707,\n",
       " 'basil': 1477,\n",
       " 'real': 12029,\n",
       " 'mint': 9540,\n",
       " 'chip': 2896,\n",
       " 'which': 16396,\n",
       " 'leaves': 8570,\n",
       " 'hubby': 7420,\n",
       " 'chocolate': 2914,\n",
       " 'guiness': 6820,\n",
       " 'four': 6089,\n",
       " 'peaks': 10866,\n",
       " 'hop': 7329,\n",
       " 'knot': 8348,\n",
       " 'best': 1657,\n",
       " 'ice': 7503,\n",
       " 'cream': 3766,\n",
       " 'phoenix': 11045,\n",
       " 'super': 14545,\n",
       " 'nice': 10041,\n",
       " 'give': 6481,\n",
       " 'us': 15879,\n",
       " 'bags': 1345,\n",
       " 'take': 14743,\n",
       " 'our': 10493,\n",
       " 'home': 7283,\n",
       " 'totally': 15327,\n",
       " 'dissapointed': 4585,\n",
       " 'had': 6868,\n",
       " 'purchased': 11785,\n",
       " 'coupon': 3676,\n",
       " 'from': 6195,\n",
       " 'travelzoo': 15435,\n",
       " 'try': 15520,\n",
       " 'out': 10496,\n",
       " 'given': 6483,\n",
       " 'its': 7980,\n",
       " 'location': 8814,\n",
       " 'would': 16606,\n",
       " 'thought': 15104,\n",
       " 'it': 7971,\n",
       " 'was': 16252,\n",
       " 'going': 6551,\n",
       " 'be': 1522,\n",
       " 'very': 16032,\n",
       " 'upscale': 15861,\n",
       " 'were': 16362,\n",
       " 'expecting': 5440,\n",
       " 'whole': 16429,\n",
       " 'lot': 8881,\n",
       " 'service': 13193,\n",
       " 'great': 6693,\n",
       " 'but': 2312,\n",
       " 'not': 10148,\n",
       " 'worth': 16601,\n",
       " 'itself': 7981,\n",
       " 'outdated': 10500,\n",
       " '80': 284,\n",
       " 'crab': 3721,\n",
       " 'cakes': 2387,\n",
       " 'appertizer': 933,\n",
       " 'cold': 3192,\n",
       " 'when': 16389,\n",
       " 'served': 13188,\n",
       " 'told': 15254,\n",
       " 'waiter': 16181,\n",
       " 'who': 16425,\n",
       " 'turn': 15556,\n",
       " 'chef': 2828,\n",
       " 'message': 9416,\n",
       " 'passed': 10776,\n",
       " 'back': 1310,\n",
       " 'she': 13272,\n",
       " 'appologized': 957,\n",
       " 'ordred': 10434,\n",
       " '12oz': 52,\n",
       " 'new': 10013,\n",
       " 'york': 16724,\n",
       " 'strip': 14358,\n",
       " 'ala': 646,\n",
       " 'carte': 2573,\n",
       " '29': 153,\n",
       " '95': 307,\n",
       " 'oz': 10602,\n",
       " 'pure': 11789,\n",
       " 'fat': 5622,\n",
       " 'price': 11555,\n",
       " 'expect': 5435,\n",
       " 'meat': 9318,\n",
       " 'than': 15015,\n",
       " 'ordered': 10428,\n",
       " 'wild': 16461,\n",
       " 'mushroom': 9844,\n",
       " 'pizza': 11174,\n",
       " 'ok': 10322,\n",
       " 'needs': 9972,\n",
       " 'an': 797,\n",
       " 'over': 10532,\n",
       " 'haul': 7014,\n",
       " 'major': 9048,\n",
       " 'way': 16300,\n",
       " 'if': 7534,\n",
       " 'want': 16220,\n",
       " 'make': 9051,\n",
       " 'any': 879,\n",
       " 'money': 9689,\n",
       " 'at': 1157,\n",
       " 'saturday': 12905,\n",
       " 'night': 10052,\n",
       " '7pm': 282,\n",
       " 'empty': 5125,\n",
       " 'think': 15082,\n",
       " 'highlight': 7186,\n",
       " 'meal': 9303,\n",
       " 'bottle': 1981,\n",
       " 'cabinet': 2365,\n",
       " 'costco': 3635,\n",
       " 'travel': 15428,\n",
       " 'recently': 12073,\n",
       " 'returned': 12444,\n",
       " 'trip': 15483,\n",
       " 'big': 1704,\n",
       " 'island': 7958,\n",
       " 'hi': 7168,\n",
       " 'arranged': 1038,\n",
       " 'throught': 15130,\n",
       " 'shopping': 13361,\n",
       " 'around': 1033,\n",
       " 'found': 6083,\n",
       " 'their': 15039,\n",
       " 'prices': 11559,\n",
       " 'phone': 11049,\n",
       " 'able': 340,\n",
       " 'all': 679,\n",
       " 'arrangements': 1040,\n",
       " 'airfair': 621,\n",
       " 'condo': 3394,\n",
       " 'car': 2505,\n",
       " 'didn': 4411,\n",
       " 'with': 16526,\n",
       " 'bs': 2197,\n",
       " 'on': 10352,\n",
       " 'hilo': 7206,\n",
       " 'adjust': 485,\n",
       " 'dates': 4053,\n",
       " 'according': 398,\n",
       " 'plan': 11196,\n",
       " 'being': 1599,\n",
       " 'accurate': 405,\n",
       " 'outstanding': 10527,\n",
       " 'value': 15939,\n",
       " 'gas': 6356,\n",
       " 'did': 4409,\n",
       " 'some': 13798,\n",
       " 'souvenier': 13885,\n",
       " 'kona': 8363,\n",
       " 'again': 573,\n",
       " 'saved': 12923,\n",
       " 've': 15968,\n",
       " 'shopped': 13358,\n",
       " 'years': 16679,\n",
       " 'becoming': 1555,\n",
       " 'groupie': 6766,\n",
       " 'been': 1567,\n",
       " 'couple': 3672,\n",
       " 'there': 15057,\n",
       " 'advantages': 520,\n",
       " 'rarely': 11967,\n",
       " 'busy': 2311,\n",
       " 'larger': 8478,\n",
       " 'groups': 6770,\n",
       " 'last': 8490,\n",
       " 'few': 5720,\n",
       " 'times': 15190,\n",
       " 'experienced': 5448,\n",
       " 'average': 1253,\n",
       " 'lousy': 8897,\n",
       " 'joining': 8119,\n",
       " 'already': 719,\n",
       " 'seated': 13085,\n",
       " 'group': 6765,\n",
       " '28': 152,\n",
       " 'high': 7181,\n",
       " 'school': 12990,\n",
       " 'hostess': 7369,\n",
       " 'wagged': 16174,\n",
       " 'her': 7142,\n",
       " 'finger': 5800,\n",
       " 'face': 5528,\n",
       " 'like': 8695,\n",
       " 'east': 4943,\n",
       " 'german': 6428,\n",
       " 'border': 1953,\n",
       " 'guard': 6800,\n",
       " 'waffle': 16169,\n",
       " 'house': 7390,\n",
       " 'waitress': 16184,\n",
       " 'poked': 11295,\n",
       " 'shoulder': 13376,\n",
       " 'attention': 1197,\n",
       " 'wife': 16449,\n",
       " 'chicken': 2855,\n",
       " 'dish': 4540,\n",
       " 'sent': 13162,\n",
       " 'as': 1080,\n",
       " 'measly': 9314,\n",
       " 'pieces': 11099,\n",
       " 'beers': 1571,\n",
       " 'tap': 14792,\n",
       " 'garbage': 6337,\n",
       " 'muffler': 9815,\n",
       " 'shop': 13354,\n",
       " 'town': 15357,\n",
       " 'shops': 13362,\n",
       " 'trust': 15513,\n",
       " 'mighty': 9482,\n",
       " 'them': 15042,\n",
       " 'greg': 6714,\n",
       " 'has': 6999,\n",
       " 'worked': 16580,\n",
       " 'multiple': 9824,\n",
       " 'cars': 2571,\n",
       " 'done': 4687,\n",
       " 'job': 8105,\n",
       " 'ever': 5333,\n",
       " 'issues': 7968,\n",
       " 'he': 7041,\n",
       " 'takes': 14748,\n",
       " 'care': 2525,\n",
       " 'problem': 11605,\n",
       " 'no': 10084,\n",
       " 'questions': 11856,\n",
       " 'asked': 1102,\n",
       " 'just': 8189,\n",
       " 'start': 14159,\n",
       " 'off': 10287,\n",
       " 'by': 2346,\n",
       " 'saying': 12940,\n",
       " 'egg': 5023,\n",
       " 'salad': 12788,\n",
       " 'sandwiches': 12862,\n",
       " 'probably': 11603,\n",
       " 'tried': 15473,\n",
       " 'sandwich': 12860,\n",
       " 'anywhere': 892,\n",
       " 'serves': 13192,\n",
       " 'sacks': 12762,\n",
       " 'far': 5600,\n",
       " 'entitled': 5217,\n",
       " 'dali': 3991,\n",
       " 'life': 8673,\n",
       " 'live': 8775,\n",
       " 'north': 10136,\n",
       " 'will': 16470,\n",
       " 'literally': 8770,\n",
       " 'many': 9120,\n",
       " 'miles': 9493,\n",
       " 'eat': 4949,\n",
       " 'top': 15293,\n",
       " 'wonderful': 16554,\n",
       " 'menu': 9396,\n",
       " 'whenever': 16390,\n",
       " 'order': 10427,\n",
       " 'comes': 3237,\n",
       " 'little': 8773,\n",
       " 'cookie': 3558,\n",
       " 'cookies': 3559,\n",
       " 'can': 2435,\n",
       " 'purchase': 11784,\n",
       " 'dough': 4722,\n",
       " 'also': 721,\n",
       " 'other': 10485,\n",
       " 'delicious': 4220,\n",
       " 'dessert': 4342,\n",
       " 'bars': 1458,\n",
       " 'salads': 12789,\n",
       " 'sale': 12796,\n",
       " 'well': 16356,\n",
       " 'eating': 4958,\n",
       " 'least': 8566,\n",
       " '14': 56,\n",
       " 'hope': 7330,\n",
       " 'never': 10012,\n",
       " 'go': 6532,\n",
       " 'away': 1275,\n",
       " 'saw': 12934,\n",
       " 'triple': 15485,\n",
       " 'so': 13738,\n",
       " 'decided': 4131,\n",
       " 'expectations': 5438,\n",
       " 'arrived': 1048,\n",
       " 'shocked': 13339,\n",
       " 'thursday': 15145,\n",
       " 'morning': 9742,\n",
       " 'line': 8724,\n",
       " 'extra': 5494,\n",
       " 'long': 8842,\n",
       " 'wait': 16179,\n",
       " 'patiently': 10810,\n",
       " 'don': 4682,\n",
       " 'needless': 9971,\n",
       " 'say': 12938,\n",
       " 'because': 1548,\n",
       " 'quickly': 11864,\n",
       " 'pork': 11358,\n",
       " 'chop': 2938,\n",
       " 'eggs': 5030,\n",
       " 'hash': 7000,\n",
       " 'browns': 2168,\n",
       " 'fabulous': 5526,\n",
       " 'toast': 15229,\n",
       " 'made': 8998,\n",
       " 'grape': 6659,\n",
       " 'jelly': 8061,\n",
       " 'recommend': 12097,\n",
       " 'anyone': 885,\n",
       " 'downtown': 4743,\n",
       " 'area': 1004,\n",
       " 'sure': 14583,\n",
       " 'bacon': 1324,\n",
       " 'hit': 7232,\n",
       " 'wine': 16492,\n",
       " 'down': 4732,\n",
       " 'wednesday': 16325,\n",
       " 'happenin': 6955,\n",
       " 'tastings': 14837,\n",
       " 'courtesy': 3685,\n",
       " 'kyot': 8401,\n",
       " 'wrong': 16639,\n",
       " 'event': 5330,\n",
       " 'minutes': 9546,\n",
       " 'maybe': 9272,\n",
       " 'spot': 14035,\n",
       " 'case': 2583,\n",
       " 'hoppin': 7339,\n",
       " 'walked': 16197,\n",
       " 'looks': 8855,\n",
       " 'possibly': 11391,\n",
       " 'manager': 9088,\n",
       " 'owner': 10590,\n",
       " 'then': 15047,\n",
       " 'pointed': 11285,\n",
       " 'towards': 15350,\n",
       " 'room': 12641,\n",
       " 'where': 16391,\n",
       " 'held': 7114,\n",
       " 'about': 343,\n",
       " 'tables': 14711,\n",
       " 'people': 10927,\n",
       " 'talk': 14757,\n",
       " 'early': 4926,\n",
       " 'bird': 1735,\n",
       " 'gets': 6435,\n",
       " 'worm': 16591,\n",
       " 'finish': 5808,\n",
       " 'table': 14709,\n",
       " 'sit': 13510,\n",
       " 'patio': 10812,\n",
       " 'said': 12778,\n",
       " 'yes': 16698,\n",
       " 'right': 12522,\n",
       " 'help': 7124,\n",
       " 'meantime': 9312,\n",
       " 'another': 850,\n",
       " 'woman': 16548,\n",
       " 'friend': 6167,\n",
       " 'took': 15283,\n",
       " 'nearby': 9954,\n",
       " '20': 107,\n",
       " 'later': 8498,\n",
       " 'looking': 8853,\n",
       " 'came': 2417,\n",
       " 'helped': 7125,\n",
       " 'yet': 16701,\n",
       " 'left': 8583,\n",
       " 'return': 12443,\n",
       " 'small': 13638,\n",
       " 'happy': 6963,\n",
       " 'hour': 7388,\n",
       " 'flier': 5905,\n",
       " 'handed': 6914,\n",
       " 'distance': 4592,\n",
       " 'even': 5327,\n",
       " 'come': 3234,\n",
       " 'know': 8351,\n",
       " 're': 12013,\n",
       " 'wondering': 16557,\n",
       " 'why': 16440,\n",
       " '15': 60,\n",
       " 'wanted': 16222,\n",
       " 'see': 13113,\n",
       " 'how': 7404,\n",
       " 'bad': 1326,\n",
       " 'frankly': 6118,\n",
       " 'same': 12833,\n",
       " 'full': 6252,\n",
       " 'detailed': 4352,\n",
       " 'menus': 9398,\n",
       " 'deals': 4093,\n",
       " 'ready': 12028,\n",
       " 'um': 15635,\n",
       " 'gave': 6375,\n",
       " '10': 16,\n",
       " 'seconds': 13096,\n",
       " 'ago': 587,\n",
       " 'enjoy': 5179,\n",
       " 'look': 8851,\n",
       " 'app': 913,\n",
       " 'specialty': 13945,\n",
       " 'appetizer': 938,\n",
       " 'much': 9807,\n",
       " 'whatever': 16378,\n",
       " 'deal': 4087,\n",
       " 'drink': 4801,\n",
       " 'section': 13103,\n",
       " 'claim': 3029,\n",
       " 'glasses': 6497,\n",
       " 'excited': 5387,\n",
       " 'wines': 16493,\n",
       " 'reality': 12031,\n",
       " 'consisted': 3467,\n",
       " 'three': 15116,\n",
       " 'different': 4427,\n",
       " 'weren': 16363,\n",
       " 'women': 16549,\n",
       " 'outside': 10525,\n",
       " 'leave': 8569,\n",
       " 'having': 7026,\n",
       " 'enough': 5189,\n",
       " 'shenanigans': 13295,\n",
       " '30': 162,\n",
       " 'since': 13482,\n",
       " 'next': 10031,\n",
       " 'door': 4699,\n",
       " 'sprouts': 14064,\n",
       " 'apps': 978,\n",
       " 'own': 10588,\n",
       " 'bet': 1660,\n",
       " 'plenty': 11250,\n",
       " 'experiences': 5449,\n",
       " 'continue': 3514,\n",
       " 'visit': 16107,\n",
       " 'happily': 6961,\n",
       " 'experience': 5447,\n",
       " 'caused': 2628,\n",
       " 'write': 16632,\n",
       " 'usually': 15894,\n",
       " 'instance': 7807,\n",
       " 'wish': 16521,\n",
       " 'actually': 447,\n",
       " 'review': 12460,\n",
       " 'poor': 11337,\n",
       " 'substandard': 14446,\n",
       " 'giving': 6486,\n",
       " 'understand': 15696,\n",
       " 'isn': 7962,\n",
       " 'better': 1667,\n",
       " 'talented': 14753,\n",
       " 'others': 10486,\n",
       " 'however': 7407,\n",
       " 'business': 2300,\n",
       " 'should': 13375,\n",
       " 'customer': 3946,\n",
       " 'cliched': 3082,\n",
       " 'less': 8626,\n",
       " 'star': 14146,\n",
       " 'oh': 10310,\n",
       " 'yeah': 16674,\n",
       " 'slapped': 13575,\n",
       " 'yelped': 16689,\n",
       " 'card': 2516,\n",
       " 'pepperoni': 10937,\n",
       " 'amazed': 749,\n",
       " 'herb': 7143,\n",
       " 'vegetable': 15976,\n",
       " 'garden': 6340,\n",
       " 'front': 6197,\n",
       " 'ingredients': 7747,\n",
       " 'these': 15065,\n",
       " 'days': 4067,\n",
       " 'dishes': 4541,\n",
       " 'cheese': 2818,\n",
       " 'platters': 11225,\n",
       " 'mouth': 9782,\n",
       " 'watering': 16285,\n",
       " 'speak': 13926,\n",
       " 'day': 4064,\n",
       " 'll': 8788,\n",
       " 'keep': 8244,\n",
       " 'adding': 469,\n",
       " 'stay': 14186,\n",
       " 'tuned': 15547,\n",
       " 'btw': 2198,\n",
       " 'decor': 4149,\n",
       " 'reminds': 12272,\n",
       " 'vig': 16066,\n",
       " 'definitely': 4188,\n",
       " 'spacious': 13898,\n",
       " 'dining': 4462,\n",
       " 'sushi': 14612,\n",
       " 'time': 15184,\n",
       " 'garlic': 6346,\n",
       " 'knots': 8350,\n",
       " 'favorite': 5643,\n",
       " 'alot': 716,\n",
       " 'panda': 10687,\n",
       " 'family': 5581,\n",
       " 'run': 12728,\n",
       " 'chinese': 2894,\n",
       " 'most': 9757,\n",
       " 'importantly': 7596,\n",
       " 'style': 14421,\n",
       " 'meals': 9304,\n",
       " 'personal': 10987,\n",
       " 'empress': 5122,\n",
       " 'sweeter': 14655,\n",
       " 'orange': 10415,\n",
       " 'flavor': 5884,\n",
       " 'melts': 9371,\n",
       " 'your': 16732,\n",
       " 'kung': 8396,\n",
       " 'pao': 10705,\n",
       " 'two': 15598,\n",
       " 'shrimp': 13403,\n",
       " 'leftovers': 8585,\n",
       " 'thanks': 15024,\n",
       " 'generous': 6407,\n",
       " 'portions': 11369,\n",
       " 'selection': 13131,\n",
       " 'sake': 12784,\n",
       " 'beer': 1570,\n",
       " 'everyone': 5340,\n",
       " 'else': 5078,\n",
       " 'checks': 2809,\n",
       " 'man': 9083,\n",
       " 'leaving': 8571,\n",
       " 'six': 13519,\n",
       " 'half': 6889,\n",
       " 'search': 13073,\n",
       " 'gotten': 6610,\n",
       " 'point': 11283,\n",
       " 'settled': 13205,\n",
       " 'decent': 4125,\n",
       " 'work': 16579,\n",
       " 'colleague': 3202,\n",
       " 'suggested': 14496,\n",
       " 'dine': 4454,\n",
       " 'corporate': 3610,\n",
       " 'meeting': 9348,\n",
       " 'while': 16398,\n",
       " 'realized': 12033,\n",
       " 'native': 9933,\n",
       " 'metro': 9435,\n",
       " 'thing': 15077,\n",
       " 'trusted': 15514,\n",
       " 'his': 7227,\n",
       " 'palate': 10656,\n",
       " 'sense': 13156,\n",
       " 'pleased': 11243,\n",
       " 'diners': 4458,\n",
       " 'predominately': 11467,\n",
       " 'asian': 1098,\n",
       " 'knew': 8337,\n",
       " 'market': 9165,\n",
       " 'things': 15079,\n",
       " 'signaled': 13445,\n",
       " 'thrilled': 15123,\n",
       " 'familiar': 5579,\n",
       " 'dim': 4448,\n",
       " 'sum': 14510,\n",
       " 'carts': 2578,\n",
       " 'rolling': 12624,\n",
       " 'through': 15128,\n",
       " 'party': 10768,\n",
       " 'shared': 13254,\n",
       " 'fantastic': 5596,\n",
       " 'succulent': 14469,\n",
       " 'orders': 10431,\n",
       " 'equally': 5244,\n",
       " 'veggies': 15981,\n",
       " 'brightly': 2120,\n",
       " 'colored': 3215,\n",
       " 'tastes': 14830,\n",
       " 'distinctive': 4598,\n",
       " 'hot': 7374,\n",
       " 'sour': 13871,\n",
       " 'soup': 13868,\n",
       " 'perfect': 10949,\n",
       " 'filled': 5766,\n",
       " 'kinds': 8310,\n",
       " 'mushrooms': 9845,\n",
       " 'etc': 5301,\n",
       " 'delighted': 4226,\n",
       " 'reminisced': 12273,\n",
       " 'dinners': 4466,\n",
       " 'chinatown': 2893,\n",
       " 'ny': 10224,\n",
       " 'comparable': 3290,\n",
       " 'close': 3108,\n",
       " 'regular': 12197,\n",
       " 'chandler': 2741,\n",
       " 'clear': 3067,\n",
       " 'side': 13428,\n",
       " 'world': 16588,\n",
       " 'figure': 5750,\n",
       " 'marco': 9133,\n",
       " 'polo': 11314,\n",
       " 'palace': 10653,\n",
       " 'prompt': 11671,\n",
       " 'rounded': 12678,\n",
       " 'hooray': 7326,\n",
       " 'stayed': 14189,\n",
       " 'hotel': 7377,\n",
       " 'tuscany': 15566,\n",
       " 'scallops': 12951,\n",
       " 'pasta': 10788,\n",
       " 'excellent': 5374,\n",
       " 'server': 13189,\n",
       " 'bliss': 1820,\n",
       " 'haven': 7024,\n",
       " 'quality': 11833,\n",
       " 'dinner': 4465,\n",
       " 'stupendous': 14413,\n",
       " 'every': 5336,\n",
       " 'cent': 2674,\n",
       " 'must': 9856,\n",
       " 'yin': 16705,\n",
       " 'yang': 16663,\n",
       " 'martini': 9195,\n",
       " 'creative': 3779,\n",
       " 'presented': 11514,\n",
       " 'smiles': 13664,\n",
       " 'smores': 13684,\n",
       " 'desert': 4315,\n",
       " 'hands': 6928,\n",
       " 'connosiuer': 3444,\n",
       " 'critical': 3821,\n",
       " 'college': 3209,\n",
       " 'kids': 8288,\n",
       " 'ra': 11887,\n",
       " 'mediocre': 9341,\n",
       " 'list': 8755,\n",
       " 'irk': 7938,\n",
       " 'during': 4891,\n",
       " 'co': 3145,\n",
       " 'workers': 16582,\n",
       " 'packed': 10616,\n",
       " 'extremely': 5503,\n",
       " 'meet': 9347,\n",
       " 'coworkers': 3708,\n",
       " 'tired': 15214,\n",
       " 'chill': 2878,\n",
       " 'past': 10787,\n",
       " 'friends': 6172,\n",
       " 'alone': 710,\n",
       " 'quite': 11877,\n",
       " 'bit': 1749,\n",
       " 'servings': 13197,\n",
       " 'obviously': 10256,\n",
       " 'low': 8911,\n",
       " 'zen': 16785,\n",
       " '32': 172,\n",
       " 'choice': 2922,\n",
       " 'sakebomber': 12785,\n",
       " 'loud': 8886,\n",
       " 'noises': 10098,\n",
       " 'trying': 15521,\n",
       " 'louder': 8887,\n",
       " 'music': 9848,\n",
       " 'person': 10985,\n",
       " 'overall': 10533,\n",
       " 'tip': 15207,\n",
       " 'priced': 11556,\n",
       " 'works': 16587,\n",
       " 'horrible': 7351,\n",
       " 'twice': 15586,\n",
       " 'gone': 6564,\n",
       " 'messing': 9420,\n",
       " 'starbucks': 14148,\n",
       " 'hard': 6969,\n",
       " 'believe': 1604,\n",
       " 'attitude': 1202,\n",
       " 'lady': 8427,\n",
       " 'doesn': 4648,\n",
       " 'ordering': 10429,\n",
       " 'someone': 13803,\n",
       " 'fix': 5846,\n",
       " 'known': 8356,\n",
       " 'ya': 16655,\n",
       " 'employees': 5117,\n",
       " 'rude': 12708,\n",
       " 'makes': 9055,\n",
       " 'anymore': 884,\n",
       " 'stopped': 14290,\n",
       " 'still': 14252,\n",
       " 'mom': 9679,\n",
       " 'goes': 6549,\n",
       " 'coffee': 3182,\n",
       " 'coast': 3151,\n",
       " 'deli': 4210,\n",
       " 'west': 16365,\n",
       " 'appalachians': 914,\n",
       " 'creams': 3771,\n",
       " 'pricy': 11566,\n",
       " 'apparently': 921,\n",
       " 'free': 6134,\n",
       " 'summer': 14515,\n",
       " 'kept': 8256,\n",
       " 'stars': 14158,\n",
       " 'frown': 6207,\n",
       " 'birthday': 1739,\n",
       " 'spirits': 13998,\n",
       " 'written': 16637,\n",
       " 'clarity': 3042,\n",
       " 'margaritas': 9140,\n",
       " 'strong': 14376,\n",
       " 'check': 2797,\n",
       " '18': 75,\n",
       " 'mahi': 9025,\n",
       " 'each': 4917,\n",
       " '24': 135,\n",
       " 'rancid': 11948,\n",
       " 'yup': 16765,\n",
       " 'fajitas': 5564,\n",
       " 'jumbo': 8173,\n",
       " 'charge': 2763,\n",
       " 'entree': 5222,\n",
       " 'pay': 10844,\n",
       " 'onions': 10360,\n",
       " 'once': 10353,\n",
       " 'week': 16329,\n",
       " 'asada': 1081,\n",
       " 'enchilada': 5138,\n",
       " 'avocado': 1260,\n",
       " 'lover': 8905,\n",
       " 'ripest': 12542,\n",
       " 'avocados': 1261,\n",
       " 'f724': 5521,\n",
       " 'bother': 1975,\n",
       " 'asking': 1104,\n",
       " 'entering': 5201,\n",
       " 'reply': 12331,\n",
       " 'spoke': 14020,\n",
       " 'acknowledged': 419,\n",
       " 'presence': 11511,\n",
       " 'ticket': 15153,\n",
       " 'happened': 6954,\n",
       " 'effort': 5020,\n",
       " 'tell': 14925,\n",
       " 'scathing': 12969,\n",
       " 'deplorable': 4288,\n",
       " 'sadly': 12768,\n",
       " 'dmv': 4628,\n",
       " 'modeled': 9641,\n",
       " 'agonizing': 588,\n",
       " 'depths': 4297,\n",
       " 'hell': 7117,\n",
       " 'musac': 9837,\n",
       " 'score': 13009,\n",
       " 'playing': 11233,\n",
       " 'voracious': 16143,\n",
       " 'stench': 14221,\n",
       " 'air': 619,\n",
       " 'spastic': 13921,\n",
       " 'children': 2869,\n",
       " 'running': 12731,\n",
       " 'version': 16028,\n",
       " 'politicians': 11310,\n",
       " 'campaigning': 2426,\n",
       " 'late': 8496,\n",
       " 'complete': 3331,\n",
       " 'emissions': 5104,\n",
       " 'test': 14990,\n",
       " 'those': 15101,\n",
       " 'dmvs': 4629,\n",
       " 'open': 10379,\n",
       " 'saturdays': 12906,\n",
       " 'map': 9121,\n",
       " 'precious': 11458,\n",
       " 'fortunate': 6071,\n",
       " 'near': 9953,\n",
       " 'seriously': 13180,\n",
       " 'entire': 5215,\n",
       " 'valley': 15936,\n",
       " 'receiving': 12071,\n",
       " 'such': 14471,\n",
       " 'magnificent': 9018,\n",
       " 'welcome': 16352,\n",
       " 'among': 781,\n",
       " 'painful': 10633,\n",
       " 'simplify': 13476,\n",
       " 'let': 8630,\n",
       " 'feel': 5678,\n",
       " 'constitutes': 3479,\n",
       " 'acceptable': 371,\n",
       " 'behavior': 1594,\n",
       " 'public': 11732,\n",
       " 'bathe': 1492,\n",
       " 'combination': 3227,\n",
       " 'water': 16280,\n",
       " 'soap': 13742,\n",
       " 'along': 711,\n",
       " 'massaging': 9221,\n",
       " 'skin': 13553,\n",
       " 'creates': 3775,\n",
       " 'outcome': 10499,\n",
       " 'pleasing': 11244,\n",
       " 'scrub': 13050,\n",
       " 'behind': 1595,\n",
       " 'ears': 4932,\n",
       " 'reference': 12142,\n",
       " 'website': 16318,\n",
       " 'notorious': 10164,\n",
       " 'http': 7416,\n",
       " 'www': 16645,\n",
       " 'craigslist': 3740,\n",
       " 'org': 10443,\n",
       " 'htf': 7414,\n",
       " '755891987': 278,\n",
       " 'html': 7415,\n",
       " 'waiting': 16183,\n",
       " 'tapping': 14800,\n",
       " 'foot': 6022,\n",
       " 'rocking': 12609,\n",
       " 'forth': 6070,\n",
       " 'chair': 2712,\n",
       " 'bouncing': 1999,\n",
       " 'leg': 8586,\n",
       " 'aren': 1006,\n",
       " 'faster': 5620,\n",
       " 'watching': 16279,\n",
       " 'clock': 3102,\n",
       " 'anxious': 877,\n",
       " 'leash': 8563,\n",
       " 'cannot': 2465,\n",
       " 'quietly': 11868,\n",
       " 'winter': 16506,\n",
       " 'fine': 5794,\n",
       " 'cute': 3953,\n",
       " 'monster': 9702,\n",
       " 'aisles': 634,\n",
       " 'honey': 7307,\n",
       " 'bun': 2261,\n",
       " 'squeezing': 14083,\n",
       " 'between': 1671,\n",
       " 'fingers': 5805,\n",
       " 'fake': 5566,\n",
       " 'falls': 5575,\n",
       " 'five': 5845,\n",
       " 'row': 12686,\n",
       " 'maintain': 9039,\n",
       " 'quiet': 11866,\n",
       " 'tones': 15272,\n",
       " 'ensure': 5197,\n",
       " 'appropriate': 973,\n",
       " 'conversations': 3541,\n",
       " 'taking': 14749,\n",
       " 'making': 9060,\n",
       " 'calls': 2404,\n",
       " 'sorry': 13851,\n",
       " 'stripping': 14366,\n",
       " 'popped': 11342,\n",
       " 'into': 7877,\n",
       " 'mind': 9515,\n",
       " 'failed': 5550,\n",
       " 'audition': 1218,\n",
       " 'le': 8539,\n",
       " 'girl': 6470,\n",
       " 'continued': 3515,\n",
       " 'conversation': 3539,\n",
       " 'woodworker': 16571,\n",
       " 'acknowledge': 418,\n",
       " 'humans': 7440,\n",
       " 'fact': 5540,\n",
       " 'sideways': 13435,\n",
       " 'disappear': 4490,\n",
       " 'quit': 11876,\n",
       " 'bumping': 2259,\n",
       " 'hitting': 7235,\n",
       " 'unless': 15770,\n",
       " 'flirting': 5917,\n",
       " 'need': 9966,\n",
       " 'level': 8636,\n",
       " 'brilliant': 2122,\n",
       " 'recipe': 12083,\n",
       " 'dashing': 4050,\n",
       " 'dreams': 4778,\n",
       " 'gutting': 6850,\n",
       " 'humanity': 7439,\n",
       " 'manage': 9084,\n",
       " 'painfully': 10634,\n",
       " 'lit': 8766,\n",
       " 'reeks': 12137,\n",
       " 'brother': 2158,\n",
       " 'old': 10328,\n",
       " 'gym': 6853,\n",
       " 'shoes': 13345,\n",
       " 'dad': 3979,\n",
       " 'dirty': 4486,\n",
       " 'underwear': 15703,\n",
       " 'award': 1271,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the vocabulary that was generated, containing 16,825 unique words.\n",
    "#   'vocabulary_' is a dictionary that converts each word to its index in the sparse matrix.\n",
    "#   - For example, the word \"four\" is index #3230 in the sparse matrix.\n",
    "\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, let's convert the sparse matrix to a typical ndarray using .toarray()\n",
    "#   - Remember, this takes up a lot more memory than the sparse matrix! However, this conversion is sometimes necessary.\n",
    "\n",
    "X_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function below for simplicity.\n",
    "\n",
    "# Define a function that accepts a vectorizer and calculates the accuracy.\n",
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print(('Features: ', X_train_dtm.shape[1]))\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print(('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 13000)\n",
      "('Accuracy: ', 0.9217221135029354)\n"
     ]
    }
   ],
   "source": [
    "# min_df ignores words that occur less than twice ('df' means \"document frequency\").\n",
    "vect = CountVectorizer(min_df=1, max_features=13000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect.transform('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3922    Looking a cutting edge, wanting the best for e...\n",
       "8379    Greatness in the form of food, just like the o...\n",
       "4266    The Flower Studio far exceeded my expectations...\n",
       "5577        So yummy! Strange combination but great place\n",
       "537     I've been hearing about these cheesecakes from...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=2, max_features=10000)\n",
    "X_train_dtm = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00          5\n",
       "000         2\n",
       "00am        2\n",
       "00pm        1\n",
       "01          1\n",
       "           ..\n",
       "zone        1\n",
       "zoo         2\n",
       "zucchini    1\n",
       "zuchinni    1\n",
       "zumba       1\n",
       "Length: 8783, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names()).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look next at other ways of preprocessing text!\n",
    "\n",
    "- **Objective:** Demonstrate common text preprocessing techniques.\n",
    "\n",
    "<a id='ngrams'></a>\n",
    "### N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are features which consist of N consecutive words. This is useful because using the bag-of-words model, treating `data scientist` as a single feature has more meaning than having two independent features `data` and `scientist`!\n",
    "\n",
    "Example:\n",
    "```\n",
    "my cat is awesome\n",
    "Unigrams (1-grams): 'my', 'cat', 'is', 'awesome'\n",
    "Bigrams (2-grams): 'my cat', 'cat is', 'is awesome'\n",
    "Trigrams (3-grams): 'my cat is', 'cat is awesome'\n",
    "4-grams: 'my cat is awesome'\n",
    "```\n",
    "\n",
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 169847)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams.\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to see how supplementing our features with n-grams can lead to more feature columns. When we produce n-grams from a document with $W$ words, we add an additional $(n-W+1)$ features (at most). That said, be careful — when we compute n-grams from an entire corpus, the number of _unique_ n-grams could be vastly higher than the number of _unique_ unigrams! This could cause an undesired feature explosion.\n",
    "\n",
    "Although we sometimes add important new features that have meaning such as `data scientist`, many of the new features will just be noise. So, particularly if we do not have much data, adding n-grams can actually decrease model performance. This is because if each n-gram is only present once or twice in the training set, we are effectively adding mostly noisy features to the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zone out', 'zone when', 'zones', 'zones dolls', 'zoning', 'zoning issues', 'zoo', 'zoo and', 'zoo is', 'zoo not', 'zoo the', 'zoo ve', 'zoyo', 'zoyo for', 'zucca', 'zucca appetizer', 'zucchini', 'zucchini and', 'zucchini bread', 'zucchini broccoli', 'zucchini carrots', 'zucchini fries', 'zucchini pieces', 'zucchini strips', 'zucchini veal', 'zucchini very', 'zucchini with', 'zuchinni', 'zuchinni again', 'zuchinni the', 'zumba', 'zumba class', 'zumba or', 'zumba yogalates', 'zupa', 'zupa flavors', 'zuzu', 'zuzu in', 'zuzu is', 'zuzu the', 'zwiebel', 'zwiebel kräuter', 'zzed', 'zzed in', 'éclairs', 'éclairs napoleons', 'école', 'école lenôtre', 'ém', 'ém all']\n"
     ]
    }
   ],
   "source": [
    "# Last 50 features\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>\n",
    "\n",
    "### Stop-Word Removal\n",
    "\n",
    "- **What:** This process is used to remove common words that will likely appear in any text.\n",
    "- **Why:** Because common words exist in most documents, they likely only add noise to your model and should be removed.\n",
    "\n",
    "**What are stop words?**\n",
    "Stop words are some of the most common words in a language. They are used so that a sentence makes sense grammatically, such as prepositions and determiners, e.g., \"to,\" \"the,\" \"and.\" However, they are so commonly used that they are generally worthless for predicting the class of a document. Since \"a\" appears in spam and non-spam emails, for example, it would only contribute noise to our model.\n",
    "\n",
    "Example: \n",
    "\n",
    "> 1. Original sentence: \"The dog jumped over the fence\"  \n",
    "> 2. After stop-word removal: \"dog jumped over fence\"\n",
    "\n",
    "The fact that there is a fence and a dog jumped over it can be derived with or without stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(ngram_range=(1, 2))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show vectorizer options.\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {`english`}, list, or None (default)\n",
    "- If `english`, a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If None, no stop words will be used. `max_df` can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. (If `max_df` = 0.7, then if > 70% of documents contain a word it will not be included in the feature set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 16528)\n",
      "('Accuracy: ', 0.9158512720156555)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': 'english',\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove English stop words.\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)\n",
    "vect.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'fill', 'seems', 'afterwards', 'now', 'thin', 'serious', 'alone', 'out', 'well', 'find', 'amount', 'own', 'its', 'from', 'along', 'however', 'bottom', 'very', 'myself', 'amongst', 'throughout', 'thick', 'indeed', 'an', 'even', 'almost', 'front', 'several', 'became', 'into', 'hasnt', 'must', 'then', 'ours', 'either', 'eight', 'when', 'here', 'interest', 'would', 'should', 'together', 'call', 'why', 'nor', 'neither', 'twenty', 'least', 'around', 'sixty', 'formerly', 'towards', 'couldnt', 'co', 'empty', 'hence', 'what', 'during', 'also', 'forty', 'before', 'whither', 'everything', 'their', 'one', 'to', 'in', 'of', 'yourself', 'go', 'him', 'system', 'enough', 'further', 'de', 'than', 'your', 'across', 'i', 'ourselves', 'anything', 'if', 'same', 'had', 'ltd', 'nothing', 'get', 'under', 'latterly', 'few', 'though', 'already', 'somehow', 'becoming', 'whence', 'upon', 'wherever', 'whose', 'whenever', 'keep', 'onto', 'whole', 'been', 'who', 'after', 'bill', 'might', 'as', 'sometime', 'made', 're', 'else', 'still', 'yours', 'these', 'namely', 'hereby', 'therein', 'describe', 'none', 'anyone', 'side', 'done', 'so', 'rather', 'where', 'nowhere', 'much', 'ie', 'found', 'cry', 'without', 'itself', 'third', 'mill', 'the', 'his', 'somewhere', 'perhaps', 'latter', 'ten', 'therefore', 'they', 'something', 'nevertheless', 'on', 'or', 'thence', 'was', 'herein', 'beyond', 'nobody', 'nine', 'meanwhile', 'often', 'others', 'against', 'sincere', 'above', 'at', 'top', 'hereafter', 'can', 'three', 'besides', 'no', 'always', 'con', 'again', 'except', 'me', 'otherwise', 'although', 'former', 'back', 'noone', 'beforehand', 'through', 'via', 'thereupon', 'my', 'herself', 'being', 'give', 'six', 'may', 'full', 'ever', 'toward', 'due', 'up', 'this', 'never', 'all', 'below', 'move', 'our', 'yourselves', 'because', 'seem', 'he', 'etc', 'mostly', 'last', 'by', 'with', 'more', 'between', 'we', 'whereafter', 'beside', 'un', 'eleven', 'each', 'four', 'thru', 'cant', 'whereas', 'could', 'which', 'off', 'over', 'whatever', 'part', 'sometimes', 'how', 'until', 'someone', 'her', 'there', 'us', 'mine', 'that', 'both', 'amoungst', 'per', 'less', 'see', 'will', 'hundred', 'is', 'put', 'such', 'hers', 'those', 'name', 'whereupon', 'becomes', 'himself', 'another', 'have', 'since', 'once', 'hereupon', 'please', 'behind', 'she', 'show', 'anywhere', 'were', 'for', 'every', 'fifty', 'whether', 'take', 'twelve', 'first', 'whereby', 'inc', 'a', 'them', 'many', 'has', 'you', 'five', 'eg', 'and', 'within', 'down', 'yet', 'become', 'themselves', 'whom', 'be', 'thus', 'about', 'some', 'thereafter', 'moreover', 'next', 'only', 'not', 'anyhow', 'elsewhere', 'other', 'thereby', 'do', 'everywhere', 'whoever', 'am', 'two', 'too', 'any', 'everyone', 'fire', 'anyway', 'among', 'seemed', 'it', 'fifteen', 'are', 'seeming', 'detail', 'while', 'most', 'cannot', 'wherein', 'but'})\n"
     ]
    }
   ],
   "source": [
    "# Set of stop words\n",
    "print((vect.get_stop_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cvec_opt'></a>\n",
    "### Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `max_features`: int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus. This allows us to keep more common n-grams and remove ones that may appear once. If we include words that only occur once, this can lead to said features being highly associated with a class and cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 100)\n",
      "('Accuracy: ', 0.8698630136986302)\n"
     ]
    }
   ],
   "source": [
    "# Remove English stop words and only keep 100 features.\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing', 'area', 'atmosphere', 'awesome', 'bad', 'bar', 'best', 'better', 'big', 'came', 'cheese', 'chicken', 'clean', 'coffee', 'come', 'day', 'definitely', 'delicious', 'did', 'didn', 'dinner', 'don', 'eat', 'excellent', 'experience', 'favorite', 'feel', 'food', 'free', 'fresh', 'friendly', 'friends', 'going', 'good', 'got', 'great', 'happy', 'home', 'hot', 'hour', 'just', 'know', 'like', 'little', 'll', 'location', 'long', 'looking', 'lot', 'love', 'lunch', 'make', 'meal', 'menu', 'minutes', 'need', 'new', 'nice', 'night', 'order', 'ordered', 'people', 'perfect', 'phoenix', 'pizza', 'place', 'pretty', 'price', 'prices', 'really', 'recommend', 'restaurant', 'right', 'said', 'salad', 'sandwich', 'sauce', 'say', 'service', 'staff', 'store', 'sure', 'table', 'thing', 'things', 'think', 'time', 'times', 'took', 'tried', 'try', 've', 'wait', 'want', 'way', 'went', 'wine', 'work', 'worth', 'years']\n"
     ]
    }
   ],
   "source": [
    "# All 100 features\n",
    "print((vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with all other models, more features does not mean a better model. So, we must tune our feature generator to remove features whose predictive capability is none or very low.\n",
    "\n",
    "In this case, there is roughly a 1.6% increase in accuracy when we double the n-gram size and increase our max features by 1,000-fold. Note that if we restrict it to only unigrams, then the accuracy increases even more! So, bigrams were very likely adding more noise than signal. \n",
    "\n",
    "In the end, by only using 16,000 unigram features we came away with a much smaller, simpler, and easier-to-think-about model which also resulted in higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-grams and 2-grams, up to 100K features:\n",
      "('Features: ', 100000)\n",
      "('Accuracy: ', 0.8835616438356164)\n",
      "\n",
      "1-grams only, up to 100K features:\n",
      "('Features: ', 16825)\n",
      "('Accuracy: ', 0.9187866927592955)\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams, and limit the number of features.\n",
    "\n",
    "print('1-grams and 2-grams, up to 100K features:')\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)\n",
    "\n",
    "print()\n",
    "print('1-grams only, up to 100K features:')\n",
    "vect = CountVectorizer(ngram_range=(1, 1), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `min_df`: Float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 43957)\n",
      "('Accuracy: ', 0.9324853228962818)\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams, and only include terms that appear at least two times.\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='textblob'></a>\n",
    "## Introduction to TextBlob\n",
    "\n",
    "You should already have downloaded TextBlob, a Python library used to explore common NLP tasks. If you haven’t, please return to [this step](#textblob_install) for instructions on how to do so. We’ll be using this to organize our corpora for analysis.\n",
    "\n",
    "As mentioned earlier, you can read more on the [TextBlob website](https://textblob.readthedocs.io/en/dev/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "# Print the first review.\n",
    "print((yelp_best_worst.text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it as a TextBlob object.\n",
    "review = TextBlob(yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the words.\n",
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"My wife took me here on my birthday for breakfast and it was excellent.\"),\n",
       " Sentence(\"The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.\"),\n",
       " Sentence(\"Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.\"),\n",
       " Sentence(\"It looked like the place fills up pretty quickly so the earlier you get here the better.\"),\n",
       " Sentence(\"Do yourself a favor and get their Bloody Mary.\"),\n",
       " Sentence(\"It was phenomenal and simply the best I've ever had.\"),\n",
       " Sentence(\"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.\"),\n",
       " Sentence(\"It was amazing.\"),\n",
       " Sentence(\"While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.\"),\n",
       " Sentence(\"It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.\"),\n",
       " Sentence(\"It was the best \"toast\" I've ever had.\"),\n",
       " Sentence(\"Anyway, I can't wait to go back!\")]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the sentences.\n",
    "review.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"my wife took me here on my birthday for breakfast and it was excellent.  the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  our waitress was excellent and our food arrived quickly on the semi-busy saturday morning.  it looked like the place fills up pretty quickly so the earlier you get here the better.\n",
       "\n",
       "do yourself a favor and get their bloody mary.  it was phenomenal and simply the best i've ever had.  i'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  it was amazing.\n",
       "\n",
       "while everything on the menu looks excellent, i had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  it came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  it was the best \"toast\" i've ever had.\n",
       "\n",
       "anyway, i can't wait to go back!\")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some string methods are available.\n",
    "review.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stem'></a>\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming is a crude process of removing common endings from sentences, such as \"s\", \"es\", \"ly\", \"ing\", and \"ed\".\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form.\n",
    "- **Why:** This intelligently reduces the number of features by grouping together (hopefully) related words.\n",
    "- **Notes:**\n",
    "    - Stemming uses a simple and fast rule-based approach.\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing).\n",
    "    - Some search engines treat words with the same stem as synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excel', 'the', 'weather', 'was', 'perfect', 'which', 'made', 'sit', 'outsid', 'overlook', 'their', 'ground', 'an', 'absolut', 'pleasur', 'our', 'waitress', 'was', 'excel', 'and', 'our', 'food', 'arriv', 'quick', 'on', 'the', 'semi-busi', 'saturday', 'morn', 'it', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretti', 'quick', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'bloodi', 'mari', 'it', 'was', 'phenomen', 'and', 'simpli', 'the', 'best', 'i', 've', 'ever', 'had', 'i', \"'m\", 'pretti', 'sure', 'they', 'onli', 'use', 'ingredi', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'was', 'amaz', 'while', 'everyth', 'on', 'the', 'menu', 'look', 'excel', 'i', 'had', 'the', 'white', 'truffl', 'scrambl', 'egg', 'veget', 'skillet', 'and', 'it', 'was', 'tasti', 'and', 'delici', 'it', 'came', 'with', '2', 'piec', 'of', 'their', 'griddl', 'bread', 'with', 'was', 'amaz', 'and', 'it', 'absolut', 'made', 'the', 'meal', 'complet', 'it', 'was', 'the', 'best', 'toast', 'i', 've', 'ever', 'had', 'anyway', 'i', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# Initialize stemmer.\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Stem each word.\n",
    "print([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are \"excellent\" stemmed to \"excel\" and \"amazing\" stemmed to \"amaz\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a more refined process that uses specific language and grammar rules to derive the root of a word.  \n",
    "\n",
    "This is useful for words that do not share an obvious root such as \"better\" and \"best\".\n",
    "\n",
    "- **What:** Lemmatization derives the canonical form (\"lemma\") of a word.\n",
    "- **Why:** It can be better than stemming.\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['best'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('best').words.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'wa', 'excellent', 'The', 'weather', 'wa', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'wa', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'wa', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredient', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'wa', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'look', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'egg', 'vegetable', 'skillet', 'and', 'it', 'wa', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'piece', 'of', 'their', 'griddled', 'bread', 'with', 'wa', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'wa', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# Assume every word is a noun.\n",
    "print([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are \"filled\" lemmatized to \"fill\" and \"was\" lemmatized to \"wa\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'take', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'be', 'excellent', 'The', 'weather', 'be', 'perfect', 'which', 'make', 'sit', 'outside', 'overlook', 'their', 'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'be', 'excellent', 'and', 'our', 'food', 'arrive', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'be', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'have', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'be', 'amaze', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'look', 'excellent', 'I', 'have', 'the', 'white', 'truffle', 'scramble', 'egg', 'vegetable', 'skillet', 'and', 'it', 'be', 'tasty', 'and', 'delicious', 'It', 'come', 'with', '2', 'piece', 'of', 'their', 'griddle', 'bread', 'with', 'be', 'amaze', 'and', 'it', 'absolutely', 'make', 'the', 'meal', 'complete', 'It', 'be', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'have', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# Assume every word is a verb.\n",
    "print([word.lemmatize(pos='v') for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are \"was\" lemmatized to \"be\" and \"arrived\" lemmatized to \"arrive\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Lemmatization and Stemming Examples**\n",
    "\n",
    "|Lemmatization|Stemming|\n",
    "|-------------|---------|\n",
    "|shouted → shout|badly → bad|\n",
    "|best → good|computing → comput|\n",
    "|better → good|computed → comput|\n",
    "|good → good|wipes → wip|\n",
    "|wiping → wipe|wiped → wip|\n",
    "|hidden → hide|wiping → wip|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Knowledge Check\n",
    "- What other words or phrases might cause problems with stemming? Why?\n",
    "- What other words or phrases might cause problems with lemmatization? Why?\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jakea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jakea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns a list of lemmas.\n",
    "def split_into_lemmas(text):\n",
    "    text = text.lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 16389)\n",
      "('Accuracy: ', 0.9197651663405088)\n"
     ]
    }
   ],
   "source": [
    "# Use split_into_lemmas as the feature extraction function (Warning: SLOW!).\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas, decode_error='replace')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yuyuyummy', 'yuzu', 'z', 'z-grill', 'z11', 'zach', 'zam', 'zanella', 'zankou', 'zappos', 'zatsiki', 'zen', 'zen-like', 'zero', 'zero-star', 'zest', 'zexperience', 'zha', 'zhou', 'zia', 'zilch', 'zin', 'zinburger', 'zinburgergeist', 'zinc', 'zinfandel', 'zing', 'zip', 'zipcar', 'zipper', 'zipps', 'ziti', 'zoe', 'zombi', 'zombie', 'zone', 'zoning', 'zoo', 'zoyo', 'zucca', 'zucchini', 'zuchinni', 'zumba', 'zupa', 'zuzu', 'zwiebel-kräuter', 'zzed', 'éclairs', 'école', 'ém']\n"
     ]
    }
   ],
   "source": [
    "# Last 50 features\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the available options for `CountVectorizer()`, you may wonder how to decide which to use! It's true that you can sometimes reason about which preprocessing techniques might work best. However, you will often not know for sure without trying out many different combinations and comparing their accuracies. \n",
    "\n",
    "> Keep in mind that you should constantly be thinking about the result of each preprocessing step instead of blindly trying them without thinking. Does each type of preprocessing \"makes sense\" with the input data you are using? Is it likely to keep intact the signal and remove noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tfidf'></a>\n",
    "## Term Frequency–Inverse Document Frequency (TF–IDF)\n",
    "\n",
    "While a Count Vectorizer simply totals up the number of times a \"word\" appears in a document, the more complex TF-IDF Vectorizer analyzes the uniqueness of words between documents to find distinguishing characteristics. \n",
    "     \n",
    "- **What:** Term frequency–inverse document frequency (TF–IDF) computes the \"relative frequency\" with which a word appears in a document, compared to its frequency across all documents.\n",
    "- **Why:** It's more useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents).\n",
    "- **Notes:** It's used for search-engine scoring, text summarization, and document clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term frequency\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term frequency–inverse document frequency (simple version)\n",
    "tf/df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the TF–IDF value, the more \"important\" the word is to that specific document. Here, \"cab\" is the most important and unique word in document 1, while \"please\" is the most important and unique word in document 2. TF–IDF is often used for training as a replacement for word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More details:** [TF–IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yelp_tfidf'></a>\n",
    "## Using TF–IDF to Summarize a Yelp Review\n",
    "\n",
    "Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF–IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28880)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a document-term matrix using TF–IDF.\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit transform Yelp data.\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    \n",
    "    # Choose a random review that is at least 300 characters.\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = yelp.text[review_id]\n",
    "        #review_text = unicode(yelp.text[review_id], 'utf-8')\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # Create a dictionary of words and their TF–IDF scores.\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # Print words with the top five TF–IDF scores.\n",
    "    print('TOP SCORING WORDS:')\n",
    "    top_scores = sorted(list(word_scores.items()), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print(word, score)\n",
    "    \n",
    "    # Print five random words.\n",
    "    print(('\\n' + 'RANDOM WORDS:'))\n",
    "    random_words = np.random.choice(list(word_scores.keys()), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print(word)\n",
    "    \n",
    "    # Print the review.\n",
    "    print(('\\n' + review_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "cocoajoe 0.22960053245074902\n",
      "plaintain 0.22960053245074902\n",
      "overflavored 0.22960053245074902\n",
      "neem 0.22960053245074902\n",
      "repaint 0.22960053245074902\n",
      "\n",
      "RANDOM WORDS:\n",
      "days\n",
      "giant\n",
      "rice\n",
      "bar\n",
      "did\n",
      "\n",
      "I didn't really \"get it\".\n",
      "I was expecting more Island style food, or more adventurous treats.\n",
      "\n",
      "*  Bar was great.\n",
      "*  Tuna Poke thing was fantastic.\n",
      "*  Cajun burger was terrific, but the bun was too dense and troublesome.\n",
      "*  Chicken CocoaJoe was a bust.  Chicken a little dry and under-seasoned, rice and beans were bland (needs a giant ham-hock and about 3 more days).  The Plaintain was overflavored, but underseasoned (they don't neem much more than salt and time).\n",
      "\n",
      "Awesome location.\n",
      "Menu will expand I am sure.\n",
      "Repaint the bathroom please.\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sentiment'></a>\n",
    "## Sentiment Analysis\n",
    "\n",
    "Understanding how positive or negative a review is. There are many ways in practice to compute a sentiment value. For example:\n",
    "\n",
    "- Have a list of \"positive\" words and a list of \"negative\" words and count how many occur in a document. \n",
    "- Train a classifier given many examples of \"positive\" documents and \"negative\" documents. \n",
    "    - Note that this technique is often just an automated way to derive the first (e.g., using bag-of-words with logistic regression, a coefficient is assigned to each word!).\n",
    "\n",
    "For the most accurate sentiment analysis, you will want to train a custom sentiment model based on documents that are particular to your application. Generic models (such as the one we are about to use!) often do not work as well as hoped.\n",
    "\n",
    "As we will do below, always make sure you double-check that the algorithm is working by manually verifying that scores correctly correspond to positive/negative reviews! Otherwise, you may be using numbers that are not accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40246913580246907"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polarity ranges from -1 (most negative) to 1 (most positive).\n",
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  length  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0     889  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understanding the apply method\n",
    "yelp['length'] = yelp.text.apply(len)\n",
    "yelp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns the polarity.\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "    #return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  length  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0     889  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0    1345  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0      76  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0     419  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0     469  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame column for sentiment (Warning: SLOW!).\n",
    "yelp['sentiment'] = yelp.text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x3fde5bc8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Box plot of sentiment grouped by stars\n",
    "yelp.boxplot(column='sentiment', by='stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254     Our server Gary was awesome. Food was amazing....\n",
       "347     3 syllables for this place. \\nA-MAZ-ING!\\n\\nTh...\n",
       "420                                     LOVE the food!!!!\n",
       "459     Love it!!! Wish we still lived in Arizona as C...\n",
       "679                                      Excellent burger\n",
       "                              ...                        \n",
       "9165    My family has been going to Dr. Brittan for 10...\n",
       "9218    I've been coming here for years...the best.  O...\n",
       "9559    It's the best!  Downstairs, hanging at the bar...\n",
       "9744    Kick @$$ The Best of the bertos & they will ma...\n",
       "9989    Great food and service! Country food at its best!\n",
       "Name: text, Length: 68, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviews with most positive sentiment\n",
    "yelp[yelp.sentiment == 1].text.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773     This was absolutely horrible. I got the suprem...\n",
       "1517                  Nasty workers and over priced trash\n",
       "3266    Absolutely awful... these guys have NO idea wh...\n",
       "4766                                       Very bad food!\n",
       "5812        I wouldn't send my worst enemy to this place.\n",
       "9924                                    Horrible service.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviews with most negative sentiment\n",
    "yelp[yelp.sentiment == -1].text.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widen the column display.\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>106JT5p8e8Chtd0CZpcARw</td>\n",
       "      <td>2009-08-06</td>\n",
       "      <td>KowGVoP_gygzdSu6Mt3zKQ</td>\n",
       "      <td>5</td>\n",
       "      <td>RIP AZ Coffee Connection.  :(  I stopped by two days ago unaware that they had closed.  I am severely bummed.  This place is irreplaceable!  Damn you, Starbucks and McDonalds!</td>\n",
       "      <td>review</td>\n",
       "      <td>jKeaOrPyJ-dI9SNeVqrbww</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>-0.302083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>57-dgZzOnLox6eudArRKgw</td>\n",
       "      <td>2008-08-28</td>\n",
       "      <td>sksXE8krD3WvqSOhtlSUyQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Obsessed. Like, I've-got-the-Twangy-Tart-withdrawal-shakes level of addiction to this place. Please make one in Arcadia! Pleeeaaassse.</td>\n",
       "      <td>review</td>\n",
       "      <td>gEnU4BqTK-4abqYl_Ljjfg</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>134</td>\n",
       "      <td>-0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>PwtYeGu-19v9bU4nbP9UbA</td>\n",
       "      <td>2011-12-05</td>\n",
       "      <td>8yfOlQGxQlCgQL9TnnzQkw</td>\n",
       "      <td>5</td>\n",
       "      <td>Unfortunately Out of Business.</td>\n",
       "      <td>review</td>\n",
       "      <td>0fOPM1H03gF5EJooYvkL1Q</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3516</th>\n",
       "      <td>Bc4DoKgrKCtCuN-0O5He3A</td>\n",
       "      <td>2009-12-19</td>\n",
       "      <td>-qqrl4101KbQKIdar1lMRw</td>\n",
       "      <td>5</td>\n",
       "      <td>Cashew brittle, almond brittle, bacon brittle!  Go now, before it's too late!</td>\n",
       "      <td>review</td>\n",
       "      <td>wHg1YkCzdZq9WBJOTRgxHQ</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>-0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6726</th>\n",
       "      <td>FURgKkRFtMK5yKbjYZVVwA</td>\n",
       "      <td>2012-08-13</td>\n",
       "      <td>8xx8i94sKvBhWZv8ZVyfBA</td>\n",
       "      <td>5</td>\n",
       "      <td>Brown bag chicken sammich, mac n cheese, fried okra, and the bourbon drink.  Nuff said.</td>\n",
       "      <td>review</td>\n",
       "      <td>hFP7Si9jvdOUmmMesg4ghw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9809</th>\n",
       "      <td>_FXql6eVhbM923RdCi94SA</td>\n",
       "      <td>2009-12-08</td>\n",
       "      <td>tRpCw6R3dXwzf9q2Q0Bhyg</td>\n",
       "      <td>5</td>\n",
       "      <td>I have to tell you....\\n\\nI had their Jerk Chicken Plate the other night, and it was yuuuuummmmmyyy! You gotta try it!</td>\n",
       "      <td>review</td>\n",
       "      <td>_uL7OiQSfNsCd60DrAf7qQ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "      <td>-0.397656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars  \\\n",
       "390   106JT5p8e8Chtd0CZpcARw  2009-08-06  KowGVoP_gygzdSu6Mt3zKQ      5   \n",
       "1287  57-dgZzOnLox6eudArRKgw  2008-08-28  sksXE8krD3WvqSOhtlSUyQ      5   \n",
       "3075  PwtYeGu-19v9bU4nbP9UbA  2011-12-05  8yfOlQGxQlCgQL9TnnzQkw      5   \n",
       "3516  Bc4DoKgrKCtCuN-0O5He3A  2009-12-19  -qqrl4101KbQKIdar1lMRw      5   \n",
       "6726  FURgKkRFtMK5yKbjYZVVwA  2012-08-13  8xx8i94sKvBhWZv8ZVyfBA      5   \n",
       "9809  _FXql6eVhbM923RdCi94SA  2009-12-08  tRpCw6R3dXwzf9q2Q0Bhyg      5   \n",
       "\n",
       "                                                                                                                                                                                 text  \\\n",
       "390   RIP AZ Coffee Connection.  :(  I stopped by two days ago unaware that they had closed.  I am severely bummed.  This place is irreplaceable!  Damn you, Starbucks and McDonalds!   \n",
       "1287                                           Obsessed. Like, I've-got-the-Twangy-Tart-withdrawal-shakes level of addiction to this place. Please make one in Arcadia! Pleeeaaassse.   \n",
       "3075                                                                                                                                                   Unfortunately Out of Business.   \n",
       "3516                                                                                                    Cashew brittle, almond brittle, bacon brittle!  Go now, before it's too late!   \n",
       "6726                                                                                          Brown bag chicken sammich, mac n cheese, fried okra, and the bourbon drink.  Nuff said.   \n",
       "9809                                                           I have to tell you....\\n\\nI had their Jerk Chicken Plate the other night, and it was yuuuuummmmmyyy! You gotta try it!   \n",
       "\n",
       "        type                 user_id  cool  useful  funny  length  sentiment  \n",
       "390   review  jKeaOrPyJ-dI9SNeVqrbww     1       0      0     175  -0.302083  \n",
       "1287  review  gEnU4BqTK-4abqYl_Ljjfg     3       3      5     134  -0.625000  \n",
       "3075  review  0fOPM1H03gF5EJooYvkL1Q     0       2      0      30  -0.500000  \n",
       "3516  review  wHg1YkCzdZq9WBJOTRgxHQ     9       8      6      77  -0.375000  \n",
       "6726  review  hFP7Si9jvdOUmmMesg4ghw     0       0      0      87  -0.600000  \n",
       "9809  review  _uL7OiQSfNsCd60DrAf7qQ     1       1      1     116  -0.397656  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negative sentiment in a 5-star review\n",
    "yelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>53YGfwmbW73JhFiemNeyzQ</td>\n",
       "      <td>2012-06-22</td>\n",
       "      <td>Gi-4O3EhE175vujbFGDIew</td>\n",
       "      <td>1</td>\n",
       "      <td>If you like the stuck up Scottsdale vibe this is a good place for you. The food isn't impressive. Nice outdoor seating.</td>\n",
       "      <td>review</td>\n",
       "      <td>Hqgx3IdJAAaoQjvrUnbNvw</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>3Srfy_VeCgwDbo4iyUFOtw</td>\n",
       "      <td>2006-08-23</td>\n",
       "      <td>K8tXedC2NMBEZ8p77zg23Q</td>\n",
       "      <td>1</td>\n",
       "      <td>My co-workers and I refer to this place as \"Pizza n' Ants\".  The staff will be happy to serve you with bare hands, right after using the till.  Also, as the nickname suggests, there has been a noticable insect problem. \\n\\n\\n\\nAs if that could all be overlooked, the pizza isn't even good.  If you are in this part of town, go to Z Pizza or Slices for great pizza instead!</td>\n",
       "      <td>review</td>\n",
       "      <td>rPGZttaVjRoVi3GYbs62cg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>368</td>\n",
       "      <td>0.567143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5257</th>\n",
       "      <td>cXx-fHY11Se8rFHkkUeaUg</td>\n",
       "      <td>2009-10-27</td>\n",
       "      <td>2yHyr0N_XNZggmIfZ7JaHw</td>\n",
       "      <td>1</td>\n",
       "      <td>Remember how I said that the Trivia was the best thing about this place?  Well, they got rid of long time Triva host, Dave (who had been featured in the College Times and was the best thing about the trivia).  Without Dave's personality, this place just doesn't cut it.  Will never go here again. Bummer.</td>\n",
       "      <td>review</td>\n",
       "      <td>nx2PS25Qe3MCEFUdO_XOtw</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6222</th>\n",
       "      <td>fDZzCjlxaA4OOmnFO-i0vw</td>\n",
       "      <td>2012-07-09</td>\n",
       "      <td>F5aRE4oqmHthiHudmnShLQ</td>\n",
       "      <td>1</td>\n",
       "      <td>My mother always told me, if I didn't have anything nice to say, say nothing!</td>\n",
       "      <td>review</td>\n",
       "      <td>J92bzxYVmyoLHULzh9xNCA</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6702</th>\n",
       "      <td>77oW-QeIXbUoTbUbrdD2aA</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>oVYk9Gxa3TY63FAeoeCEzg</td>\n",
       "      <td>1</td>\n",
       "      <td>Most livable city my eye!\\nPlastic yuppies around every corner looking for a reason to belong.  I can't wait for the homosexuals to take control of this dog park and give it some class.\\n\\nAvoid at all cost.</td>\n",
       "      <td>review</td>\n",
       "      <td>ek4GWXatDshMorJwGC2JAw</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>204</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8833</th>\n",
       "      <td>iHmfkYeEsIxbAqEj3dloQQ</td>\n",
       "      <td>2012-07-03</td>\n",
       "      <td>N_hL1-fyunhVpDDX6fz9Sg</td>\n",
       "      <td>1</td>\n",
       "      <td>The owner has changed hands &amp; this place isn't what it used to be.  If you want up to date paper &amp; quality product...go to Scrap Happy OR Crop Girls!</td>\n",
       "      <td>review</td>\n",
       "      <td>HY9A-ShZQ1MvdFvEhNY4LQ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars  \\\n",
       "1781  53YGfwmbW73JhFiemNeyzQ  2012-06-22  Gi-4O3EhE175vujbFGDIew      1   \n",
       "2353  3Srfy_VeCgwDbo4iyUFOtw  2006-08-23  K8tXedC2NMBEZ8p77zg23Q      1   \n",
       "5257  cXx-fHY11Se8rFHkkUeaUg  2009-10-27  2yHyr0N_XNZggmIfZ7JaHw      1   \n",
       "6222  fDZzCjlxaA4OOmnFO-i0vw  2012-07-09  F5aRE4oqmHthiHudmnShLQ      1   \n",
       "6702  77oW-QeIXbUoTbUbrdD2aA  2012-01-05  oVYk9Gxa3TY63FAeoeCEzg      1   \n",
       "8833  iHmfkYeEsIxbAqEj3dloQQ  2012-07-03  N_hL1-fyunhVpDDX6fz9Sg      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "1781                                                                                                                                                                                                                                                               If you like the stuck up Scottsdale vibe this is a good place for you. The food isn't impressive. Nice outdoor seating.   \n",
       "2353  My co-workers and I refer to this place as \"Pizza n' Ants\".  The staff will be happy to serve you with bare hands, right after using the till.  Also, as the nickname suggests, there has been a noticable insect problem. \\n\\n\\n\\nAs if that could all be overlooked, the pizza isn't even good.  If you are in this part of town, go to Z Pizza or Slices for great pizza instead!   \n",
       "5257                                                                      Remember how I said that the Trivia was the best thing about this place?  Well, they got rid of long time Triva host, Dave (who had been featured in the College Times and was the best thing about the trivia).  Without Dave's personality, this place just doesn't cut it.  Will never go here again. Bummer.   \n",
       "6222                                                                                                                                                                                                                                                                                                         My mother always told me, if I didn't have anything nice to say, say nothing!   \n",
       "6702                                                                                                                                                                       Most livable city my eye!\\nPlastic yuppies around every corner looking for a reason to belong.  I can't wait for the homosexuals to take control of this dog park and give it some class.\\n\\nAvoid at all cost.   \n",
       "8833                                                                                                                                                                                                                                 The owner has changed hands & this place isn't what it used to be.  If you want up to date paper & quality product...go to Scrap Happy OR Crop Girls!   \n",
       "\n",
       "        type                 user_id  cool  useful  funny  length  sentiment  \n",
       "1781  review  Hqgx3IdJAAaoQjvrUnbNvw     0       1      2     119   0.766667  \n",
       "2353  review  rPGZttaVjRoVi3GYbs62cg     0       1      0     368   0.567143  \n",
       "5257  review  nx2PS25Qe3MCEFUdO_XOtw     2       4      0     304   0.650000  \n",
       "6222  review  J92bzxYVmyoLHULzh9xNCA     1       2      1      77   0.750000  \n",
       "6702  review  ek4GWXatDshMorJwGC2JAw     1       2      4     204   0.625000  \n",
       "8833  review  HY9A-ShZQ1MvdFvEhNY4LQ     0       1      0     149   1.000000  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Positive sentiment in a 1-star review\n",
    "yelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the column display width.\n",
    "pd.reset_option('max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='add_feat'></a>\n",
    "## Bonus: Adding Features to a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will add additional features to our `CountVectorizer()`-generated feature set to hopefully improve our model.\n",
    "\n",
    "To make the best models, you will want to supplement the auto-generated features with new features you think might be important. After all, `CountVectorizer()` typically lowercases text and removes all associations between words. Or, you may have metadata to add in addition to just the text.\n",
    "\n",
    "> Remember: Although you may have hundreds of thousands of features, each data point is extremely sparse. So, if you add in a new feature, e.g., one that detects if the text is all capital letters, this new feature can still have a huge effect on the model outcome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
    "X = yelp_best_worst[feature_cols]\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064, 16825)\n",
      "(1022, 16825)\n"
     ]
    }
   ],
   "source": [
    "# Use CountVectorizer with text column only.\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "print((X_train_dtm.shape))\n",
    "print((X_test_dtm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of other four feature columns\n",
    "X_train.drop('text', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast other feature columns to float and convert to a sparse matrix.\n",
    "extra = sp.sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\n",
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16829)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine sparse matrices.\n",
    "X_train_dtm_extra = sp.sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 16829)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat for testing set.\n",
    "extra = sp.sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\n",
    "X_test_dtm_extra = sp.sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9246575342465754\n"
     ]
    }
   ],
   "source": [
    "# Use logistic regression with text column only.\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9207436399217221\n"
     ]
    }
   ],
   "source": [
    "# Use logistic regression with all features.\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm_extra)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='more_textblob'></a>\n",
    "## Bonus: Fun TextBlob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spelling correction\n",
    "TextBlob('15 minuets late').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', 0.9929478138222849), ('parrot', 0.007052186177715092)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spellcheck\n",
    "Word('parot').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tip laterally',\n",
       " 'enclose with a bank',\n",
       " 'do business with a bank or keep an account at a bank',\n",
       " 'act as the banker in a game or in gambling',\n",
       " 'be in the banking business',\n",
       " 'put into a bank account',\n",
       " 'cover with ashes so to control the rate of burning',\n",
       " 'have confidence or faith in']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definitions\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Language identification\n",
    "TextBlob('Hola amigos').detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bayes\"></a>\n",
    "\n",
    "## Appendix: Intro to Naive Bayes and Text Classification\n",
    "\n",
    "Later in the course, we will explore in-depth how to use the Naive Bayes classifier with text. Naive Bayes is a very popular classifier because it has minimal storage requirements, is fast, can be tuned easily with more data, and has found very useful applications in text classificaton. For example, Paul Graham originally proposed using Naive Bayes to detect spam in his [Plan for Spam](http://www.paulgraham.com/spam.html).\n",
    "\n",
    "Earlier we experimented with text classification using a Naive Bayes model. What exactly are Naive Bayes classifiers? \n",
    "\n",
    "**What is Bayes?**  \n",
    "Bayes, or Bayes' Theorem, is a different way to assess probability. It considers prior information in order to more accurately assess the situation.\n",
    "\n",
    "**Example:** You are playing roulette.\n",
    "\n",
    "As you approach the table, you see that the last number the ball landed on was Red-3. With a frequentist mindset, you know that the ball is just as likely to land on Red-3 again given that every slot on the wheel has an equal opportunity of 1 in 37.\n",
    "\n",
    "Given that you started believing that the ball can land in each slot with an equal likelihood _and_ that you have only seen one throw previously, you rationally believe that there would be no difference between picking Red a second time now or picking Black -- ideally they would happen with the same likelihood!\n",
    "\n",
    "However, as you sit and watch the roulette table, you begin to notice something strange. The ball is _always_ landing on red. Every single time the ball is thrown, it lands in a red slot. Even though your past beliefs stated that red and black were equally likely, every time it lands in red, you change those beliefs a little more towards a biased roulette table. \n",
    "\n",
    "This is what Bayes is all about — adjusting probabilities as more data is gathered!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the equation for Bayes.  \n",
    "\n",
    "$$P(A \\ | \\ B) = \\frac {P(B \\ | \\ A) \\times P(A)} {P(B)}$$\n",
    "\n",
    "- **$P(A \\ | \\ B)$** : Probability of `Event A` occurring given `Event B` has occurred.\n",
    "- **$P(B \\ | \\ A)$** : Probability of `Event B` occurring given `Event A` has occurred.\n",
    "- **$P(A)$** : Probability of `Event A` occurring.\n",
    "- **$P(B)$** : Probability of `Event B` occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Applying Naive Bayes Classification to Spam Filtering\n",
    "\n",
    "Let's pretend we have an email with three words: \"Send money now.\" We'll use Naive Bayes to classify it as **ham or spam.** (\"Ham\" just means not spam. It can include emails that look like spam but that you opt into!)\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) = \\frac {P(\\text{send money now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "By assuming that the features (the words) are conditionally independent, we can simplify the likelihood function:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {P(\\text{send} \\ | \\ spam) \\times P(\\text{money} \\ | \\ spam) \\times P(\\text{now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "Note that each conditional probability in the numerator is easily calculated directly from the training data!\n",
    "\n",
    "So, we can calculate all of the values in the numerator by examining a corpus of spam email:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {0.2 \\times 0.1 \\times 0.1 \\times 0.9} {P(\\text{send money now})} = \\frac {0.0018} {P(\\text{send money now})}$$\n",
    "\n",
    "We would repeat this process with a corpus of ham email:\n",
    "\n",
    "$$P(ham \\ | \\ \\text{send money now}) \\approx \\frac {0.05 \\times 0.01 \\times 0.1 \\times 0.1} {P(\\text{send money now})} = \\frac {0.000005} {P(\\text{send money now})}$$\n",
    "\n",
    "All we care about is whether spam or ham has the higher probability, and so we predict that the email is spam.\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The \"naive\" assumption of Naive Bayes (that the features are conditionally independent) is critical to making these calculations simple.\n",
    "- The normalization constant (the denominator) can be ignored since it's the same for all classes.\n",
    "- The prior probability is much less relevant once you have a lot of features.\n",
    "\n",
    "### Comparing Naive Bayes With Other Models\n",
    "\n",
    "Advantages of Naive Bayes:\n",
    "\n",
    "- Model training and prediction are very fast.\n",
    "- It's somewhat interpretable.\n",
    "- No tuning is required.\n",
    "- Features don't need scaling.\n",
    "- It's insensitive to irrelevant features (with enough observations).\n",
    "- It performs better than logistic regression when the training set is very small.\n",
    "\n",
    "Disadvantages of Naive Bayes:\n",
    "\n",
    "- If \"spam\" is dependent on non-independent combinations of individual words, it may not work well.\n",
    "- Predicted probabilities are not well calibrated.\n",
    "- Correlated features can be problematic (due to the independence assumption).\n",
    "- It can't handle negative features (with Multinomial Naive Bayes).\n",
    "- It has a higher \"asymptotic error\" than logistic regression.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## Conclusion\n",
    "\n",
    "- NLP is a gigantic field.\n",
    "- Understanding the basics broadens the types of data you can work with.\n",
    "- Simple techniques go a long way.\n",
    "- Use scikit-learn for NLP whenever possible.\n",
    "\n",
    "While we used SKLearn and TextBlob today, another popular python NLP library is [Spacy](https://spacy.io)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
